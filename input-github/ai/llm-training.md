<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-training.md# -->

## 事前学習 | Pre-training
- LLMのモデル開発を大きく3つのステップに分けたときの最初のステップ。
  - **ステップ1. 事前学習 ★**
  - ステップ2. 指示学習
  - ステップ3. アラインメント
- 教師なし学習。データセットは、Web上から収集した膨大なテキストデータを学習し知識を獲得するステップ。
- 具体的には、以下二つのような予測タスクをLLMに学習させる。人手でラベル付けする(=望ましいLLMの出力を作成する)必要がない。したがって、膨大なテキストデータをLLMに学習させることを実現し、大きなブレイクスルーとなった。
  - Next-token prediction: 次の単語の予測
  - Masked language modeling: ランダムにマスクされた単語の予測
- 事前学習したモデルは、基盤モデルと呼ばれる。
- 事前学習を実施するのにあたり、計算資源、データセット、LLMのパラメータ数の規模をどの程度にするか？が重要であり、その指標としてScaling lawsがある。
- 数千枚以上のGPUを使用する分散学習を行う。事前学習をどのように数千枚のGPUに分散するか？は重要な研究開発領域となっている。

## 指示学習 | Instruction tuning
- LLMのモデル開発を大きく3つのステップに分けたときの2番目のステップ。
  - ステップ1. 事前学習
  - **ステップ2. 指示学習 ★**
  - ステップ3. アラインメント
- 一般的に、教師あり学習の一種であるSFTと呼ばれる手法を用いて、基盤モデルをQ&A、翻訳、要約、センチメント分析、分類などのタスク向けにFine-tuningするステップ。
- 下記の論文が初めて"Instruction tuning"という用語を提唱した。
  - <a href="https://arxiv.org/abs/2109.01652"> Wei, Jason, et al. "Finetuned language models are zero-shot learners." arXiv preprint arXiv:2109.01652 (2021). </a>
  - 従来は、各タスク毎に別々にモデルを開発していた。あるいは、BERTなどのある共通のモデルを各タスク向けにFine-tuningするにしてもどのタスクか?を示す特殊なトークンを導入するなどして形式が統一的でなかった。
  - 本論文以降、ある1つの基盤モデルに対し、様々なタスクのLLMへの指示文と望ましい応答のペアという形式が統一的なデータセットを使ってFine-tuningすることが一般的となった。

## アラインメント | Alignment
- LLMのモデル開発を大きく3つのステップに分けたときの最後のステップ。
  - ステップ1. 事前学習
  - ステップ2. 指示学習
  - **ステップ3. アラインメント ★**
- 指示学習したLLMを人間の価値基準・倫理・安全性要件に沿って調整させるステップ。
- 一般的に、自己教師あり学習の一種であるRLHF、教師あり学習の一種であるDPOなどの手法が用いられる。
- OpenAI社のChatGPTがこのアラインメントをRLFHという強化学習ベースの手法で実施しているということで注目を集めたが、技術的詳細はクローズドで実際のところこのRLHFがどのくらい有効なのかはよくわかっていない。

## 事後学習 | Post-training
- LLMのモデル開発を大きく3つのステップに分けたとき、事前学習(1番目のステップ)の後に実施するステップという意味で、指示学習とアラインメントの総称として使われることがある。
  - ステップ1. 事前学習
  - **ステップ2. 指示学習 ★**
  - **ステップ3. アラインメント ★**

## 基盤モデル | Foundation Model
- 事前学習で膨大なテキストデータを学習したLLMのこと。
- 様々なタスク(Q&A、翻訳、要約、センチメント分析、分類)向けに指示学習できる。

## SFT | Supervised Fine-tuning
- 特にLLMの学習に関して使われる用語で、基盤モデルに対して、あるタスクのデータセット(LLMへの指示文と望ましい応答のペア)を用意し、Fine-tuningする。
- タスクとは具体的には、Q&A、翻訳、要約、センチメント分析、分類など。

## LoRA | Low-Rank Adaptation
- モデルの重みの一部に低ランク行列を追加し、微調整時のパラメータ数を抑えるPEFT手法の一種。

## PEFT | Parameter-Efficient Fine-Tuning
- 全パラメータではなく一部のみ更新することで、少数のリソースでファインチューニングを可能にする技術群。

## Instruction Tuning
- モデルにタスク指示文（instruction）を含むデータでファインチューニングし、指示に従った応答生成能力を向上させる手法。例：FLAN, T5。

## RLHF | Reinforcement Learning from Human Feedback
- 人間の好みを反映した報酬モデルを用いて、PPOなどの強化学習でLLMを微調整する手法。ChatGPTで活用。

## Self-Instruct
- 既存のLLMを用いて、自動的に命令データと模範応答を生成し、instruction tuning用のデータを自己生成する手法。

## Adapter Layers
- モデルの特定の層に小さな追加モジュールを挿入し、微調整時に更新するパラメータを限定する技術。
- 用途: PEFTの一種として、効率的なファインチューニングに使用。

## Prefix Tuning
- モデルの入力に特定の「プレフィックス」を追加し、その部分のみを学習することでモデルを微調整する手法。

## Gradient Checkpointing
- 大規模なLLMの学習処理でのメモリ使用量を削減するために、中間勾配を再計算する技術。
