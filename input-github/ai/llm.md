<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md# -->


## 大規模言語モデル| Large Language Model | LLM 
- Transformerベースのアーキテクチャを用いて、大規模なコーパスを自己教師あり学習により事前学習した自然言語生成モデル。

## 推論 | Inference	
- 学習済みモデルに対してプロンプトを与え、次のトークンまたはテキスト全体を生成させる処理。

## プロンプト | Prompt
- モデルへの入力文。コンテキストや命令、質問などを含むことが多く、Zero-shotやFew-shotにも活用される。

## デコーダー | Decoder
- Transformerにおける出力生成側のネットワーク構造で、自己回帰的にトークンを予測する。

## RMSNorm | LayerNorm
- 各レイヤーにおいて出力のスケーリングを正規化し勾配の安定性を高めるテクニック。

## 位置エンコーディング | Positional Encoding
- トークンの系列順序情報をモデルに与えるための埋め込み。絶対位置または相対位置を採用。

## アテンション | Attention
- 各トークンが他のトークンとの関連性を計算するメカニズム。Self-AttentionがTransformerの要。

## 自己回帰 | Autoregressive
- 入力の過去のトークンを使って次のトークンを逐次生成するモデル構造。GPT系列などが該当。

## コンテキスト長 | Context Length | Context Window
- モデルが一度に処理可能な最大トークン数。大きいほど長文処理能力が向上するが、アテンションの計算量が増大する。

## FlashAttention
- アテンション計算をメモリアクセス最適化により高速化したアルゴリズム。Transformerのスケーラビリティを改善。

## Mixture of Experts | MoE
- 複数のサブネットワーク（専門家）を用意し、入力ごとに一部のみを動かすことで、計算量を抑えつつモデルの能力を拡張。

## Sparse Transformer
- 全トークン間のアテンションではなく、一部の接続のみ有効にすることで、計算量とメモリ使用を削減したTransformerの変種。

## トークン | Token
- モデルの入出力単位であり、BPEやSentencePieceなどのサブワード分割により生成される。

## トークナイザー | トークナイザ | Tokenizer
- 生テキストをサブワード単位のトークン列に変換する処理器で、エンコーディングとデコーディングを担う。

## エンべディング | Embedding
- 離散トークンを連続値ベクトル空間へ写像する手法。語彙空間のワンホットベクトルを低次元密ベクトルに変換し、意味的距離の保存を目指す。
- 通常は入力エンべディングと位置エンべディングの加算で初期入力が構成される。

## Self-Attention
- 入力系列内の全トークンペアに対し、クエリ（Q）、キー（K）、バリュー（V）の内積スコアをSoftmax正規化して加重和を取るアテンション機構。Transformerの基盤であり、長距離依存関係の獲得に寄与する。

## SentencePiece 
- トークン化を言語非依存に行うためのサブワード単位トークナイザー。Unigram Language ModelまたはBPEに基づき、語彙と分割境界を学習する。スペースを専用トークンとして扱うことで前処理不要。

## BPE | Byte Pair Encoding
- 頻出ペアのマージ操作を繰り返して語彙を構成するデータ圧系サブワード分割アルゴリズム。単語の分割可能性を保ちつつ、語彙サイズと未出語処理のトレードオフを実現。

## コサイン類似度 | Cosine Similarity
- ベクトル間の角度のコサイン値で類似度を測定。エンベディング間比較に使用。	

