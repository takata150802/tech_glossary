<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md# -->

## 機械学習 | Machine Learning
- 明示的なルール記述なしにデータからパターンを学習するアルゴリズム全般。

## 強化学習 | Reinforcement Learning
- エージェントが環境からの報酬を最大化する行動方策を学習。Q学習やポリシー勾配法がある。

## 特徴量 | Feature
- 学習モデルの入力ベクトルを構成する次元。PCAやオートエンコーダーで次元削減可能。

## 次元の呪い | Curse of Dimensionality
- 特徴量の次元を増やしすぎると、機械学習モデルの精度が劣化する現象
  - 経験的に機械学習モデルの学習に必要なデータ量は特徴量の次元の指数オーダーで増大し、現実的にそのようなデータ量を集めるのが不可能となるため
  - 球面集中現象:
    - 特徴量のノルムが一様となり、意味をなさなくなるため(1万次元の特徴量のうち、1番目の次元の値が多少違っても1万次元全体のノルムには全く見えてこない)
    - 標本平均が母平均に一致しなくなるため(1万次元の特徴量のうち、1番目の次元の値だけ見ると標本平均は母平均に一致するが(=大数の法則)、1万次元の標本平均と1万次元の母平均のノルムは全く一致しない)

## 球面集中現象 | Concentration on the sphere
- 次元数の増加に伴い、ある点から見たときの他の点はその点から遠ざかり、また同じような距離に存在するようになる現象。
  - d次元の空間に、ある体積ごとに同じ数の点が分布しているとする
  - この時、ある点を中心として半径がそれぞれrとar(0<a<1)のd次元超球S1, S2を考えたとき、(S1の体積 - S2の体積)/(S1の体積)は1 - a^dとなり、dの増加にともない1に近づく
  - つまり、S1の体積のほとんどはS1とS2の超球の隙間に存在する。よって、d次元空間の点もS1とS2の超球の隙間に存在することになる。

## 分類 | Classification
- 離散ラベルを予測する教師あり学習問題。多クラスではSoftmaxとクロスエントロピーを使用。

## 回帰 | Regression
- 連続値を予測対象とする教師あり学習。損失関数としてMSEやMAEが用いられる。

## 確率的勾配降下法 | SGD
- バッチサイズ1または小バッチでパラメータ更新を行う勾配降下法。計算効率と汎化に優れる。

## 訓練データ | Training Data
- モデルのパラメータ最適化に使用されるデータセット。交差検証による分割が一般的。

## 訓練誤差 | Training Error
- 訓練データに対する損失関数の平均値。汎化誤差とのギャップが過学習の兆候。

## 汎化誤差 | Generalization Error
- 訓練データ外の新規入力に対する誤差。交差検証や正則化で抑制。

## F値 | F1 Score
- 適合率と再現率の調和平均。不均衡デタにおける分類性能のバランス指標。

## 混同行列 | Confusion Matrix
- 分類モデルの性能評価指標。適合率、再現率、F値はここから導出。

## 交検証 | Cross Validation
- データを複数分割し、訓練と検証を繰り返すことでモデルの汎化性能を推定する手法。

## 再現率 | Recall
- 実際の正例のうち、正しく予測された割合。感度とも。

## ROC曲線 | Receiver Operating Characteristic Curve
- 分類モデルの閾値を変化させたときのTPRとFPRの関係を可視化する評価標。

## Jaccard係数 | Jaccard Index
- 集合間の類似度を表す指標で、IoUとも呼ばれる。物体検出の評価にも用いられる。

## KL Divergence
- ある確率分布が別の分布かられだけ異なるかを測る非対称な距離指標。言語モデルの事前学習にも使用。

## オッズ比 | Odds Ratio
- 2つの事象の相対的な発生確率比。ロジスティック回帰での係数解釈に用いられる。

## ロジット関数 | Logit Function
- 確率値をオッズの対数に変換する写像。ロジスティック回帰の決定関数で使用。

## 尤度 | Likelihood
- 観測データの下でのパラメータの確からしさを表す関数。最尤法の基礎。

## 最尤法 | Maximum Likelihood Estimation
- 尤度関数を最大化するパラメータ推定法。多くの統計モデルの学習基盤。

## 標本誤差 | Sampling Error
- 母集団と標本の間で推定値が異なる誤差。交差検証で統計的に評価可能。

## 標準化 | Standardization
- 特徴量のスケールを平均0、分散1に正規化する前処理。勾配安定性と収束性向上に寄与。

## 次元削減 | Dimensionality Reduction
- 高次元特徴空間を情報保持しつつ低次元へ射影する処理。PCA、t-SNE、UMAPなど。

## 正規化 | Regularization
- モデルの過学習を防ぐため、損失関数にペナルティ項を加える手法。L1, L2正則化など。

## 自己組織化マップ | Self-Organizing Map
- 教師なし学習による高次元データの位相保存写像。Kohonenマップとも。

## 適合率 | Precision
- 予測した正例のうち、実際に正しかった割合。F値に影響。

## 重回帰 | Multiple Regression
- 複数の説明変数から連続目的変数を予測する回帰手法。

## EMアルゴリズム | Expectation-Maximization
- 隠れ変数のある確率モデルのパラメータ推定手法で、EステップとMステップを交互に適用。

## サポートベクターマシン | SVM
- カーネルトリックを利用して高次元空間でマージン最大化を行う線形分類器。

## カーネルトリック | Kernel Trick
- 非線形特徴変換を暗黙的に高次元空間で内計算する手法。SVMやPCAで活用。

## ランダムフォレスト | Random Forest
- 多数の定木をバギングにより構築し、多数決で出力を得るアンサンブル学習法。
