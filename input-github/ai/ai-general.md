<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md# -->

## 人工知能 | Artificial Intelligence | AI　
- 人間の知的な行動を模倣するコンピュータシステム
- 2010代~2024年現在まで続くAIブームは第3次AIブーム(3rd Wave)とされている。
  - First Wave (1950s-1970s):
    - GOFAI（Good Old Fashioned AI；古き良き人工知能）
    - 現在に至るAI研究の基礎となるフレームワークが輩出:
      - シンボリックAIの一般問題解決器GPS（General
Problem Solver）
      - ニューラルネットワークにおけるパーセプトロン
        - 引用元: https://ja.wikipedia.org/wiki/甘利俊一
          - `甘利俊一は連続体力学、情報理論、ニューラルネットワークなどを研究してきた。1967年、多層パーセプトロンの確率的勾配降下法を考えて定式化に成功したが、この早すぎた発見は当時の計算機の能力の低さもあり検証が難しく、あまり注目されずに終わった。しかし、1986年にデビッド・ラメルハート、ジェフリー・ヒントン、ロナルド・J・ウィリアムスが、この方法を再発見し、誤差逆伝播法として発表した事で、ニューラルネットワーク研究の第2次ブームへと繋がっている。勾配消失問題などの技術的困難があり、この第2次ブームは終焉を迎えたが、その後のディープラーニングブームへと続く礎にもなった。`
      - フレーム問題
      - 意味ネットワーク, フレーム理論
      - ファジー集合
      - 自然言語処理システム SHRDLU
      - 人工無能システム ELIZA
      - 人工知能用プログラミング言語 LISP
      - 第1回人工知能国際会議 IJCAI 1969 @ワシントン 
    - この後最初の「AI の冬」時代へ
  - Second Wave (1980s-1990s):
    - 人間の専門家の知識を持った AI が専門家のような推論を行うエキスパートシステム（Expert System）
      - 人間の専門家の知識を引き出すタスクは知識エンジニアリングと呼ばれ，知識エンジニアという職業まで生まれた
    - 病気の診断を行う MYCIN（マイシン）
    - 油田の推定を行うディップメーターアドバイザー
    - コンピュータの構成を行う XCON（エックスコン）
    - 日本 通産省: 第五世代コンピュータプロジェクト,1982年~
      - 引用元: https://museum.ipsj.or.jp/computer/other/0002.html
        - `通産省は1982年に第五世代コンピュータプロジェクトをスタートさせた．このプロジェクトは，国際貢献を果たしつつ技術先進国として発展するという我が国の政策のもとに始められたもので，国際的にみても創造的・先駆的な技術という意味を込めて「第五世代コンピュータ」と名付けられた．技術目標を「知識情報処理を指向した新しいコンピュータ技術の研究開発」と定め，推進母体として新世代コンピュータ技術開発機構（ICOT）が設立され，技術目標に含まれる多くの要素技術の実証・評価を行うために，並列推論型コンピュータのプロトタイプシステムの試作が行われた．このシステムは，知識情報処理指向のコンピュ−タとしては，世界最高速，かつ，最大規模のものであった．このシステムは，並列推論マシン（PIM）と呼ばれる大規模な並列ハードウェアシステムを持ち，PIM は512台の要素プロセッサからなるPIM/pや256台の要素プロセッサを持つPIM/mなど5つのモデルが作られた．11年間で約540億を投じ1992年度をもって終了した．現在，PIM/pおよびPIM/mは国立科学博物館で保存されている．また，下記のWebで「第五世代博物館」が公開されている．`
          - <a href="http://www.jipdec.or.jp/archives/icot/ARCHIVE/HomePage-J.html">http://www.jipdec.or.jp/archives/icot/ARCHIVE/HomePage-J.html</a>
    - 暗黙知の顕在化によるエキスパートシステムの限界などにより，15 年に及ぶ AI 冬の時代へ。この間における主要な進歩は下記の通り：
      - ベイジアンネットワーク | Bayesian Network
      - 遺伝的アルゴリズム | Genetic Algorithm
      - 遺伝的プログラミング  | Genetic Programming
      - マルチエージェントシステム | Multi-agent System
      - 強化学習
      - 統計的機械学習, サポ―トベクターマシン
      - データマイニング | Data Minig
  - Third Wave (2010s-Present):
    - コンピューター ハードウェアの進歩(とGPGPUの活用)と大規模なデータセットによるDeep Learningのブレークスルー
      - 画像認識コンペティションILSVRC2012でトロント大のAlexNetがエラー率 15.3% で優勝し、次点よりも 10.8% 以上低くDeep Learningが広く認知されるきっかけとなった。
        - `Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "ImageNet classification with deep convolutional neural networks." Communications of the ACM 60.6 (2017): 84-90.`
        - ILSVRC2012の次点は東大知能機械 原田研究室の牛久祥孝氏。「ヒントンに敗れた男」として日本経済新聞の記事で紹介されるもお名前がtypo…
    - Google、Meta(旧Facebook)、Microsoft、Amazon等の北米IT大手が研究開発を牽引し、多くのプロダクト・サービスにAIが投入され商業的成功を収めている

## 機械学習 | Machine Learning
- 明示的なルール記述なしにデータからパターンを学習するアルゴリズム全般。

## 強化学習 | Reinforcement Learning
- エージェントが環境からの報酬を最大化する行動方策を学習。Q学習やポリシー勾配法がある。

## 特徴量 | Feature
- 学習モデルの入力ベクトルを構成する次元。PCAやオートエンコーダーで次元削減可能。

## 次元の呪い | Curse of Dimensionality
- 特徴量の次元を増やしすぎると、機械学習モデルの精度が劣化する現象
  - 経験的に機械学習モデルの学習に必要なデータ量は特徴量の次元の指数オーダーで増大し、現実的にそのようなデータ量を集めるのが不可能となるため
  - 球面集中現象:
    - 特徴量のノルムが一様となり、意味をなさなくなるため(1万次元の特徴量のうち、1番目の次元の値が多少違っても1万次元全体のノルムには全く見えてこない)
    - 標本平均が母平均に一致しなくなるため(1万次元の特徴量のうち、1番目の次元の値だけ見ると標本平均は母平均に一致するが(=大数の法則)、1万次元の標本平均と1万次元の母平均のノルムは全く一致しない)

## 球面集中現象 | Concentration on the sphere
- 次元数の増加に伴い、ある点から見たときの他の点はその点から遠ざかり、また同じような距離に存在するようになる現象。
  - d次元の空間に、ある体積ごとに同じ数の点が分布しているとする
  - この時、ある点を中心として半径がそれぞれrとar(0<a<1)のd次元超球S1, S2を考えたとき、(S1の体積 - S2の体積)/(S1の体積)は1 - a^dとなり、dの増加にともない1に近づく
  - つまり、S1の体積のほとんどはS1とS2の超球の隙間に存在する。よって、d次元空間の点もS1とS2の超球の隙間に存在することになる。

## 分類 | Classification
- 離散ラベルを予測する教師あり学習問題。多クラスではSoftmaxとクロスエントロピーを使用。

## 回帰 | Regression
- 連続値を予測対象とする教師あり学習。損失関数としてMSEやMAEが用いられる。

## 確率的勾配降下法 | SGD
- バッチサイズ1または小バッチでパラメータ更新を行う勾配降下法。計算効率と汎化に優れる。

## 訓練データ | Training Data
- モデルのパラメータ最適化に使用されるデータセット。交差検証による分割が一般的。

## 訓練誤差 | Training Error
- 訓練データに対する損失関数の平均値。汎化誤差とのギャップが過学習の兆候。

## 汎化誤差 | Generalization Error
- 訓練データ外の新規入力に対する誤差。交差検証や正則化で抑制。

## F値 | F1 Score
- 適合率と再現率の調和平均。不均衡デタにおける分類性能のバランス指標。

## 混同行列 | Confusion Matrix
- 分類モデルの性能評価指標。適合率、再現率、F値はここから導出。

## 交検証 | Cross Validation
- データを複数分割し、訓練と検証を繰り返すことでモデルの汎化性能を推定する手法。

## 再現率 | Recall
- 実際の正例のうち、正しく予測された割合。感度とも。

## ROC曲線 | Receiver Operating Characteristic Curve
- 分類モデルの閾値を変化させたときのTPRとFPRの関係を可視化する評価標。

## Jaccard係数 | Jaccard Index
- 集合間の類似度を表す指標で、IoUとも呼ばれる。物体検出の評価にも用いられる。

## KL Divergence
- ある確率分布が別の分布かられだけ異なるかを測る非対称な距離指標。言語モデルの事前学習にも使用。

## オッズ比 | Odds Ratio
- 2つの事象の相対的な発生確率比。ロジスティック回帰での係数解釈に用いられる。

## ロジット関数 | Logit Function
- 確率値をオッズの対数に変換する写像。ロジスティック回帰の決定関数で使用。

## 最小二乗法 | Least Square Method  
- 目的変数Yを説明変数Xの線形モデルで表したときの二乗誤差($=\epsilon^2$)を最小にするパラメータ$\theta$を求める手法
- 線形モデルの誤差$\epsilon$を多変量正規分布と仮定したときの最尤推定に相当する

```math
\begin{align}
線形モデル&: Y = X\theta + \epsilon \\ 
目的変数&: Y \\
説明変数&: X \\
パラメータ&: \theta \\
誤差&: \epsilon \\
推定値&: \hat{\theta} = (X^TX)^{-1}X^TY ・・・①
\end{align}
```
**① 推定値 $\hat{\theta}$の導出:**

```math
\begin{align}
&\begin{split}
\epsilon^2 &=(Y-X\theta)^T(Y-X\theta) \\
&=Y^TY - Y^TX\theta - \theta^TX^TY + \theta^TX^TX\theta \\
\end{split} \\
&\begin{split}
\frac{\partial }{\partial \theta} \epsilon^2 &= \frac{\partial }{\partial \theta} Y^TY - \frac{\partial }{\partial \theta} Y^TX\theta - \frac{\partial }{\partial \theta} \theta^TX^TY + \frac{\partial }{\partial \theta} \theta^TX^TX\theta \\
&= 0 + \frac{\partial }{\partial \theta} Y^TX\theta - \frac{\partial }{\partial \theta} (X^TY)^T\theta + \frac{\partial }{\partial \theta} \theta^TX^TX\theta \\
&= -2 \frac{\partial }{\partial \theta} Y^TX\theta + \frac{\partial }{\partial \theta} \theta^TX^TX\theta ・・・② \\
&= -2X^TY+2X^TX\theta ・・・③ \\
\end{split}
\end{align}
```

したがって、$\frac{\partial }{\partial \theta} \epsilon^2 = 0$となるような$\theta$は、
```math
\begin{align}
0 &= -2X^TY+2X^TX\theta \\
X^TX\theta &= X^TY \\
(X^TX)^{-1}X^TX\theta &= (X^TX)^{-1}X^TY \\
\theta &= (X^TX)^{-1}X^TY ・・・上述の①の式
\end{align}
```

なお②から③の式変形は、
```math
\begin{align}
第１項&: - 2\frac{\partial }{\partial \theta} Y^TX\theta = - 2 (Y^TX)^T = -2 X^TY \\
第2項&: \frac{\partial }{\partial \theta} \theta^TX^TX\theta = 2X^TX\theta 
\end{align}
```

## 尤度 | 尤度関数 | Likelihood | Likehood function

- 観測値がxとなる確率を何らかの確率モデルp(x; θ)で表現しようとしている状況を考える。
- 例えば観測値として{x_1, x_2, ..., x_n}が得られたとき、何らかの確率モデルp(x; θ)が本当に正しいなら、この観測値{x_1, x_2, ..., x_n}が確率モデルのパラメータθの下で生じる確率は下記式で書ける。これが尤度。

```math
尤度: L(X = \{x_1, x_2, ..., x_n\};\theta) = \Pi_{i=1}^N p(x_i;\theta) 
```

- 尤度とは**確率モデルのパラメータθの尤もらしさ**を表す関数。具体的には、確率モデルとそのパラメータθが本当に正しいなら実際に観測された観測値系列の発生確率がどれくらいになるかを計算する関数。これは、パラメータθが尤もらしいなら今回の観測値系列の発生確率(=尤度)は実際に観測されたものであるので当然高くなるはずで、逆に尤度が低ければそのパラメータθは今回の観測値系列を上手く表現できておらず尤もらしくないという考え方に基づいている。

**具体例1:**
- 観測値はコイン投げの裏表で、コインが表になる確率を下記の確率モデルp(x; θ)で表現しようとしている状況を考える。
- 例えば観測値として{裏, 表}が得られたとき、下記の確率モデルp(x; θ)が本当に正しいなら、この観測値{裏, 表}が確率モデルのパラメータθの下で生じる確率は下記式で書ける(=尤度)。

```math
確率モデル: p(x; \theta) = 
\begin{cases}
  \theta  \text{\ \ \ \ \ \ \ \ ※\ x\ ==\ 表の場合} \\\
  1 - \theta \text{※\ x\ ==\ 裏の場合}
\end{cases}
```

```math
尤度: L(X=\{裏, 表\};\theta) = p(裏;\theta) \cdot p(表;\theta) = (1 -\theta) \cdot \theta
```
- なお、上記の尤度を最大化するのはθ=0.5である(0≦θ≦1で(1-θ)θはθ=0.5で最大となるので)。

**具体例2:**
- 観測値はLLMの出力トークンで、LLMがある出力トークンを生成する確率を確率モデルLLM(入力トークン列; 出力トークン; θ)で表現しようとしている状況を考える。
  - θはLLMの重みパラメータとなる。
- 例えば観測値として{日本の首都は?→東京, 日本の首都は?東京→です。}が得られたとき、確率モデルLLM(入力トークン列; 出力トークン; θ)が本当に正しいなら、この観測値{日本の首都は?→東京, 日本の首都は?東京→です。}が確率モデルのパラメータθの下で生じる確率は下記式で書ける。

```math
尤度: L(X=\{日本の首都は?→東京, 日本の首都は?東京→です。\};\theta) \\\
= LLM(日本の首都は?;東京;\theta) \cdot LLM(日本の首都は?東京;です。;\theta)
```

- なお、上記の尤度を最大化するθを求めるのがLLMの学習に相当する。ただ、代数的に求めることができないので数値解析的に求める必要があり、具体的には誤差逆伝播法が用いられる。

## 最尤推定 | 最尤推定法 | Maximum Likelihood Estimation
- 観測値がxとなる確率を何らかの確率モデルp(x; θ)で表現しようとしている状況で、最適なパラメータθを推定する手法。
- 具体的には、観測値系列{x_1, x_2, ..., x_n}が得られたとき、確率モデルp(x; θ)とそのパラメータθが正しいなら、その観測値系列の発生確率は確率モデルのパラメータθの関数となる(=尤度)。

```math
尤度: L(X = \{x_1, x_2, ..., x_n\};\theta) = \Pi_{i=1}^N p(x_i;\theta) 
```

- 最尤推定とは、この尤度を最大化するパラメータθの値を以ってθの推定値とする手法。
- これは、パラメータθが尤もらしいなら今回の観測値系列の発生確率(=尤度)は実際に観測されたものであるので当然高くなるはずで、逆に尤度が低ければそのパラメータθは今回の観測値系列を上手く表現できておらず尤もらしくないという考え方に基づいている。

## 標本誤差 | Sampling Error
- 母集団と標本の間で推定値が異なる誤差。交差検証で統計的に評価可能。

## 標準化 | Standardization
- 特徴量のスケールを平均0、分散1に正規化する前処理。勾配安定性と収束性向上に寄与。

## 次元削減 | Dimensionality Reduction
- 高次元特徴空間を情報保持しつつ低次元へ射影する処理。PCA、t-SNE、UMAPなど。

## 正規化 | Regularization
- モデルの過学習を防ぐため、損失関数にペナルティ項を加える手法。L1, L2正則化など。

## 自己組織化マップ | Self-Organizing Map
- 教師なし学習による高次元データの位相保存写像。Kohonenマップとも。

## 適合率 | Precision
- 予測した正例のうち、実際に正しかった割合。F値に影響。

## 重回帰 | Multiple Regression
- 複数の説明変数から連続目的変数を予測する回帰手法。

## EMアルゴリズム | Expectation-Maximization
- 隠れ変数のある確率モデルのパラメータ推定手法で、EステップとMステップを交互に適用。

## サポートベクターマシン | SVM
- カーネルトリックを利用して高次元空間でマージン最大化を行う線形分類器。

## カーネルトリック | Kernel Trick
- 非線形特徴変換を暗黙的に高次元空間で内計算する手法。SVMやPCAで活用。

## ランダムフォレスト | Random Forest
- 多数の定木をバギングにより構築し、多数決で出力を得るアンサンブル学習法。

## バイアス | Bias
- モデルの予測値の平均と真の値との差。
- 複雑なモデルを使用するとバイアスは低下するが、バリアンスが増加する傾向がある。

## バリアンス | Variance
- モデルの予測値の分散。
- 学習データセットの変動に対するモデルの感度を示す。
- バリアンスが高い場合、単一の学習データセットに過学習している可能性がある。

## 削減不能な誤差 | Irreducible Error
- データの測定誤差などに由来するノイズの分散。
- 削減できない。

## 条件付き確率 | Conditional Probability
- ある事象が起こったときの別の事象の確率。
```math
  Bが起こったときのAの条件付き確率: P(A|B) = \frac{P(A \cap B)}{P(B)}
```
```math
  なお、事象AとBが独立ならP(A \cap B) = P(A)P(B)、つまりP(A|B) = P(A)という簡単な話になる。そして、独立でない場合を考えたいとき、P(A \cap B) = P(A|B)P(B)を満たすP(A|B)なる確率が必要になり、このP(A|B)を条件付き確率と呼んでいるという話。
```

## 同時確率| Joint probability
- 複数の事象が同時に起きる確率
```math
事象AとBの同時確率: P(A \cap B)
```

## 条件付き確率の連鎖律 | Chain rule
- 同時確率は、条件付き確率の積で表現できる。
```math
  事象AとBとCの同時確率: P(A \cap B \cap C) = P(A|B \cap C)P(B \cap C) = P(A|B \cap C)P(B|C)P(C)
```
# 参考文献
- 朱鷺の杜Wiki https://ibisforest.org/index.php?FrontPage
- https://www.kamishima.net/archive/mldm-overview.pdf
- https://www.ieice-hbkb.org/files/S3/S3gun_03hen_01.pdf
- https://twitter.com/losnuevetoros/status/1168326617023700992
