<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-ethics-safety.md# -->

## ハルシネーション | Hallucination
- 入力に存在しない情報をあたかも事実のように生成する、生成モデル特有の誤り。

## Prompt Injection	
- プロンプト内に意図的な命令を埋め込み、モデルの出力意図的に改変させる攻撃手法。セキュリティリスクの一種。

## アライメント | Alignment
- モデル出力が人間の意図や社会的規範と整合するよう調整・訓練する取り組み全般。RLHFや安全性評価が含まれる。

## ガードレール | Guardrails
- モデル出力のセーフティやポリシー制御を目的としたルールやフィルタ。ハルシネーション回避、有害発言の抑制、特定タスク制限などの用途に活用される。