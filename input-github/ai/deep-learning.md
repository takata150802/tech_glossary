<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md# -->

## 深層学習 | Deep Learning
- 多層のニューラルネットワークを用いて**データの階層的特徴表現を学習**する機械学習の一分野。
- 人手で特徴量設計する手法に対し、深層学習は**特徴量抽出から予測までをエンドツーエンドで最適化**する。

- 主なモデル:
  * **DNN (Deep Neural Network)**: 全結合層の多層ネットワーク。
  * **CNN (Convolutional Neural Network)**: 画像認識、ResNet, EfficientNet。
  * **RNN (Recurrent Neural Network)**: 系列データ、LSTM, GRU。
  * **Transformer**: 自己注意機構による並列処理。GPT, BERT, ViT。
- 応用分野:
  * **自然言語処理 (NLP)**: 機械翻訳、要約、対話（LLM）。
  * **コンピュータビジョン (CV)**: 画像分類、物体検出、セグメンテーション。
  * **音声処理**: 音声認識、音声合成。
  * **マルチモーダル**: CLIP, GPT-4V などでテキスト＋画像＋音声の統合。
  * **科学・工学**: 蛋白質構造予測 (AlphaFold)、自動運転、HPC最適化。

## ニューラルネットワーク | Neural Network

- 人間の脳神経回路に着想を得て設計された非線形関数近似モデルで、複雑なパターン認識や関数近似を行う。
- 以下の式で表せる**層**を積み重ねた構造になっていて、ある層の出力が次の層への入力となる。

$$
h^{(l)} = f\left(W^{(l)} h^{(l-1)} + b^{(l)}\right)
$$

  * $h^{(l)}$: 層 $l$ の出力
  * $W^{(l)}$: 重み行列
  * $b^{(l)}$: バイアス
  * $f$: 活性化関数（ReLU, GELU, etc.）

- 従来の統計学的手法 単回帰、重回帰、線形分類モデルは、一つの層から構成されるニューラルネットワーク、すなわち単層ニューラルネットワーク（single-layer neural network）と見なすことができる。これらは、線形分離可能な問題しか扱えない。
- 一方で、ニューラルネットワークには万能近似能力がある。
  - 二層以上のニューラルネットワークで、
  - 活性化関数が非線形で、
  - 中間層(=1層目の出力 すなわち2層目の入力)の次元が無限で、
  - 訓練データが無限にあれば、
  - どんな関数でも近似できるというもの。
- (参考):
  - https://ibis.t.u-tokyo.ac.jp/suzuki/lecture/2020/intensive2/Kyusyu_2020_Deep.pdf
  -  https://chokkan.github.io/mlnote/classification/03nn.html



## 活性化関数 | 伝達関数 | Activation Function
- ニューラルネットワークを構成要素の1つ。
  - 典型的には、ニューラルネットワークは複数の層から構成されており、
  - ある層の出力が次の層への入力となっていて、
  - ある層の出力は、
    - ある層の入力xに対し**線形変換** (重み付き和 + バイアス) した後に、
    - **非線形関数**を通したものとして計算される。この非線形関数を活性化関数をと呼ぶ。
- 活性化関数には、ニューラルネットワークが線形分離可能でない問題を扱えるようにするという重要の役割がある(→詳細はニューラルネットワークの節を参照)。

* **具体例**:
  1. **Sigmoid**
     $\sigma(x)=\frac{1}{1+e^{-x}}$

     * 範囲 (0,1)。古典的。勾配消失が起きやすい。
  2. **tanh**
     $\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

     * 範囲 (-1,1)。
  3. **ReLU (Rectified Linear Unit)**
     $f(x)=\max(0,x)$
     * 現代の標準。計算効率が高い。
  4. **GELU (Gaussian Error Linear Unit)**
     $f(x)=x\cdot \Phi(x)$ （$\Phi$ は標準正規分布のCDF）
     * Transformer系モデル（BERT, GPT）で採用。ReLUより滑らか。
  5. **SiLU/Swish**
     $f(x)=x \cdot \sigma(x)$
     * GPT-4, PaLM などで利用。

## 重み | 重みパラメータ | Weight | Weight Parameter
- ニューラルネットワーク内部の**学習可能な係数（行列・ベクトル・バイアス）の総称**。
- 誤差逆伝播法で最適化される対象。

## ファインチューニング | Fine-tuning
- 既に学習済みモデルの重みを初期値として、別のタスクに適用させるべく追加でのデータセットで追加学習すること。
  - 画像分類モデルの専門領域適応(一般的な画像分類モデルを車両分類モデルに特化させる)
  - LLMの専門領域適応（医学、法律、金融など）
  - LLMの応答のスタイル調整（対話口調、企業ポリシー適合）
- 種類:
  - **フルファインチューニング:** 全重みパラメータを更新する。
  - **部分ファインチューニング|フリーズ:** 特定の重みパラメータだけを更新する
  - **PEFT:** アダプターと呼ばれる追加の重みパラメータを用意し、そのアダプターだけ学習する

## フリーズ | Freeze
- ファインチューニングの文脈では、部分ファインチューニングやPEFTにおいて、ファインチューニング対象の深層学習モデルの重みパラメータの一部をファインチューニングで更新しないように固定することを指して、フリーズと呼ぶ。
- 例えば「出力層以外の重みパラメータをフリーズする。」というように用いられる。

## 学習率 | Learning Rate
- 勾配降下法において、各ステップでパラメータをどれだけ更新するかを調整するハイパーパラメータ。
- 下記式で、学習率$\eta$が大きすぎると発散し、小さすぎると収束が遅くなる。
  - ※ 発散する・・・学習率が大きすぎると、最適解を飛び越えてしまい、損失関数が減少せずにむしろ増加してしまう現象。
  - ※ 収束する・・・学習が進むにつれて損失関数の値が徐々に減少し、最適解に近づいていくこと。

$$
\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)
$$
* $\eta$: 学習率
* $\nabla_\theta L(\theta)$: 勾配

## Optimizer | 最適化手法
- 損失関数の勾配を基に、モデルの重みパラメータを更新するアルゴリズム。様々なアルゴリズムが考案されている。
- 具体例:
  1. **SGD (Stochastic Gradient Descent)**
     * 最も基本。$\eta$は学習率。
       $$
       \theta \leftarrow \theta - \eta \nabla_\theta L(\theta)
       $$
  2. **Momentum SGD**
     * 勾配の移動平均を利用して収束を加速。
  3. **Adam (Adaptive Moment Estimation)**
     * \*\*1次モーメント（平均）と2次モーメント（分散）\*\*を組み合わせる。
     * LLMで最も一般的（特に AdamW = Adam + Weight Decay）。
  4. **AdaGrad, RMSProp, Lion** なども存在。


## Scheduler | 学習率スケジューラ
- 学習率 (Learning Rate) を **訓練ステップに応じて動的に変化させる仕組み**。様々な手法が考案されている。
- 具体例:
  * **Step Decay**: 一定ステップごとに学習率を減少。
  * **Exponential Decay**: 学習率を指数的に減少。
  * **Cosine Annealing**: 学習率をコサインカーブで減少。
  * **Warmup + Decay**: 初期は徐々に増加（Warmup）、その後減衰。

## Dropout | ドロップアウト
- ニューラルネットワークの学習中に、ある層の出力yの各要素について確率 $p$ でニューロンを無効化する正則化手法。
- 過学習 (overfitting) を防ぎ、汎化性能を向上する効果がある。
- 学習中だけ適用され、推論時にはドロップアウトは無効化される。

$$
h_i' = \begin{cases} 
  0 & \text{with prob } p \\ 
  \frac{h_i}{1-p} & \text{with prob } 1-p
\end{cases}
$$

## 自己教師あり学習 | Self-supervised Learning
- ラベルなしデータから部分的にタスクを構成し、教師あり学習のように学習する手法。

## 勾配降下法 | Gradient Descent
- 誤差関数の勾配を計算し、負の方向へパラメータを更新する最適化アルゴリズム。バッチサイズに応じてSGD、Mini-batch、Full-batchなどに分かれ、変種としてAdamやRMSpropがある。

## プーリング層 | Poolingレイヤー | Pooling Layer
- 畳み込み後の特徴マップを空間的にダウンサンプリングする操作。MaxPoolingやAveragePoolingが主流。

## エポック | Epoch
- 訓練データ全体を1回通して学習するサイクル。過学習回避のため早期終了と併用。

## バッチサイズ | Batch Size
- 1回のパラメータ更新に使うサンプル数。メモリ使用と学習安定性に影響。

## リカレントニューラルネットワーク | RNN
- 系列データに適したネットワークで、隠れ状態を時間的に伝播。勾配消失の影響を受けやすい。

## 勾配消失問題 | Vanishing Gradient Problem
- 活性化関数や深い層の影響で勾配が消失し、学習が進行しなくなる現象。ReLUや残差接続で緩和可能。

## 変分オートエンコーダー | VAE
- 潜在変数モデルをニューラルネットワークで構成し、ELBOの最小化により訓練される生成モデル。

## 拡散モデル | Diffusion Model
- ノイズ付加と復元の2過程でデータ生成を行う確率的生成モデル。DDPMやStable Diffusionが代表。

## 損失関数 | Loss Function
- モデルの予測と正解ラベルの「誤差」を定量化する関数。
- **学習の目的**となる。すなわち、勾配降下法の最小化対象。
- 具体例：
  1. **分類タスク**: Cross-Entropy 損失
     $$
     L = -\sum_{i} y_i \log p_\theta(y_i \mid x)
     $$
     * 大規模言語モデルでもCross-Entropy損失が標準。次のトークンの予測は、大規模な語彙の中から1つを選ぶ分類タスクなので。
  2. **回帰タスク**: MSE (平均二乗誤差)、MAE (平均絶対誤差)。
  3. **ランキングや強化学習**: RLHF(e.g. PPO, DPO)では報酬モデルに基づく損失。

## 教師あり学習 | Supervised Learning
- 入力と正解ラベルの対を用いた学習手法。分類や回帰が該当。

## 敵対的生成ネットワーク | GAN
- 生成器と識別器がミニマックスゲームを行う構造を持つ生成モデル。潜在空間から高品質サンプル生成が可能。

## 物体検出 | Object Detection
- 画像内の対象物を局所化（バウンディングボックス）し、分類も行う複合タスク。YOLOやFaster R-CNNが代表。

## 畳み込みニューラルネットワーク | CNN
- 空間的局所性とパラメータ共有を活用し、画像や時系列データの処理に特化したディープネットワーク。

## 誤差逆伝播法 | バックプロパゲーション | Backpropagation
- チェインルールにより勾配を層ごとに計算し、重み更新に利用。

## 転移学習 | Transfer Learning
- 事前学習済みモデルの知識を別タスクに応用する学習パラダイム。ファインチューニングと併用。

## 交差検証 | Cross-Validation

- 機械学習モデル、特に深層学習モデルの教師あり学習において、訓練中に過学習を検出する手法の一つ。
- 典型的には、学習データを訓練データと検証データに分割し、
  - 訓練データを使って訓練を1エポック~数エポック進めた後(各エポックでTraining lossを算出する)、
  - 検証データを使ってValidation lossを算出する。
  - Training lossは順当に低下し続けるもののValidation lossが上昇に転じた時点で過学習と判断する。
- なお、検証データを使ってValidation lossを算出するのを何エポックに一回にするか？は訓練ループの設定パラメータ。 

## 過学習 | Overfitting
- モデルが**訓練データに過剰に適合**しすぎてしまい、未知のデータ（検証・テストデータ）に対する汎化性能が低下する現象。
- 実践的には、学習が進むにつれTraining lossは順当に低下しているものの、Validation lossが増加に転じる現象を言う。
* **対策**:
  - 学習データがモデルのパラメータに対し少なすぎる
  - 学習データにノイズが多い(=学習に適さないデータがある分実質的な学習データが減る)
  - 学習データにバイアスがある(=Training dataとValidation dataが乖離している)
  - Dropout
  - Weight Decay
  - Data Augmentation
  - Early Stopping

## 汎化性能 | Generalization Performance
- 学習済みモデルが**訓練データ以外の未知データ**に対しても適切に予測できる能力のこと。
- 機械学習の最終目標は「訓練データへの適合」ではなく「未知データへの予測精度の最大化」であり、この性質を **汎化 (Generalization)** と呼ぶ。

## Early Stopping | 早期終了
- 深層学習モデルの訓練で**過学習が始まった時点で訓練を打ち切る**手法。
  - 典型的には、深層学習モデルの訓練を進めていくと、
    - **訓練誤差 (Training Loss)** は一貫して低下する。
    - **検証誤差 (Validation Loss)** はある点を境に上昇し始める（過学習）。
- 実践的には、Validation lossが改善しなくった時点で訓練を打ち切れば良い。
  - **Patience パラメータ**: 検証誤差が改善しない状態が *N* エポック続いたら訓練を打ち切る。
  - **チェックポイント保存**: 検証誤差を評価する度に訓練途中のモデルの重みを保存しておき、最後に検証誤差が最小だったモデルの重みを採用する。

## 音声認識 | Automatic Speech Recognition | ASR
- 音響特徴量から系列ラベル（文字列）への変換問題。典型的にはCTC損失やRNN-Tを使用。

## コサイン類似度 | Cosine Similarity
- 2つのベクトルがどれだけ同じ方向を向いているかを数値で表す指標。
- 具体的には、2つのベクトル $u, v$ のコサイン類似度とは、 $u, v$のなす角の余弦(下記式)。
  - 方向の近さを評価する。ベクトルの大きさは考慮しない。
  $$
  \cos(u,v) = \frac{u \cdot v}{\|u\|\|v\|}
  $$

- TF-IDFやword2vecなどの手法である文章をベクトル化した結果(=Embedding)の比較に用いられる。

## ハイパーパラメータ | Hyperparameter

- 機械学習の分野(=深層学習を含む)で使われる用語で、主に**モデルの学習過程の設定パラメータ**を指す。
- **モデルの構造をとある設定パラメータで若干調整できるようにしている場合にそのパラメータ**も含めてハイパーパラメータと呼ぶこともある。
- 訓練データから直接学習されるのではなく、人間が事前に設定する。
- 具体例:
  1. **学習率 (Learning Rate)**
  2. **バッチサイズ (Batch Size)**
  3. **エポック数 (Epochs)**
  4. **モデル構造**
  5. **正則化パラメータ**
  6. **Optimizerの設定**
  7. **ランダムシード**