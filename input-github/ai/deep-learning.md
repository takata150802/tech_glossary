<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md# -->

## 重み | 重みパラメータ | Weight | Weight Parameter
- モデル内部で学習される重みおよびバイアスで、誤差逆伝播法で最適化される対象。

## ファインチューニング | Fine-tuning
- 事前学習済みモデルに対し、特定のタスクやドメインに合わせて勾配更新を行うトレーニング。

## 学習率 | Learning Rate
- 勾配降下法において、各ステップでパラメータをどれだけ更新するかを制御するハイパーパラメータ。

## 自己教師あり学習 | Self-supervised Learning
- ラベルなしデータから部分的にタスクを構成し、教師あり学習のように学習する手法。

## 勾配降下法 | Gradient Descent
- 誤差関数の勾配を計算し、負の方向へパラメータを更新する最適化アルゴリズム。バッチサイズに応じてSGD、Mini-batch、Full-batchなどに分かれ、変種としてAdamやRMSpropがある。

## プーリング層 | Poolingレイヤー | Pooling Layer
- 畳み込み後の特徴マップを空間的にダウンサンプリングする操作。MaxPoolingやAveragePoolingが主流。

## エポック | Epoch
- 訓練データ全体を1回通して学習するサイクル。過学習回避のため早期終了と併用。

## ドロップアウト | Dropout
- ニューラルネットワークの過学習防止のために、学習時ランダムにユニットを無効化する正則化手法。

## ハイパーパラメータ | Hyperparameter
- 訓練前に設定する調整変数で、バッチサイズ、学習率、ドロップアウト率などが含まれる。

## バッチサイズ | Batch Size
- 1回のパラメータ更新に使うサンプル数。メモリ使用と学習安定性に影響。

## リカレントニューラルネットワーク | RNN
- 系列データに適したネットワークで、隠れ状態を時間的に伝播。勾配消失の影響を受けやすい。

## 勾配消失問題 | Vanishing Gradient Problem
- 活性化関数や深い層の影響で勾配が消失し、学習が進行しなくなる現象。ReLUや残差接続で緩和可能。

## 変分オートエンコーダー | VAE
- 潜在変数モデルをニューラルネットワークで構成し、ELBOの最小化により訓練される生成モデル。

## 拡散モデル | Diffusion Model
- ノイズ付加と復元の2過程でデータ生成を行う確率的生成モデル。DDPMやStable Diffusionが代表。

## 損失関数 | Loss Function
- モデル出力と教師信号との誤差を数値化する指標で、勾配降下法の最小化対象。代表例にクロスエントロピーやMSE。

## 教師あり学習 | Supervised Learning
- 入力と正解ラベルの対を用いた学習手法。分類や回帰が該当。

## 敵対的生成ネットワーク | GAN
- 生成器と識別器がミニマックスゲームを行う構造を持つ生成モデル。潜在空間から高品質サンプル生成が可能。

## 物体検出 | Object Detection
- 画像内の対象物を局所化（バウンディングボックス）し、分類も行う複合タスク。YOLOやFaster R-CNNが代表。

## 畳み込みニューラルネットワーク | CNN
- 空間的局所性とパラメータ共有を活用し、画像や時系列データの処理に特化したディープネットワーク。

## 自然言語処理 | NLP
- 言語的構造を持つ非構造データを扱う機械学習分野。トークン化、エンベディング、Attention機構を含む。

## 誤差逆伝播法 | バックプロパゲーション | Backpropagation
- チェインルールにより勾配を層ごとに計算し、重み更新に利用。

## 転移学習 | Transfer Learning
- 事前学習済みモデルの知識を別タスクに応用する学習パラダイム。ファインチューニングと併用。

## 過学習 | Overfitting
- 訓練誤差が小さいが汎化誤差が大きい状態。正則化やデータ拡張で緩和可能。

## 音声認識 | Automatic Speech Recognition | ASR
- 音響特徴量から系列ラベル（文字列）への変換問題。典型的にはCTC損失やRNN-Tを使用。