<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md# -->

## 深層学習 | Deep Learning
- 多層のニューラルネットワークを用いて**データの階層的特徴表現を学習**する機械学習の一分野。
- 人手で特徴量設計する手法に対し、深層学習は**特徴量抽出から予測までをエンドツーエンドで最適化**する。

- 主なモデル:
  * **DNN (Deep Neural Network)**: 全結合層の多層ネットワーク。
  * **CNN (Convolutional Neural Network)**: 画像認識、ResNet, EfficientNet。
  * **RNN (Recurrent Neural Network)**: 系列データ、LSTM, GRU。
  * **Transformer**: 自己注意機構による並列処理。GPT, BERT, ViT。
- 応用分野:
  * **自然言語処理 (NLP)**: 機械翻訳、要約、対話（LLM）。
  * **コンピュータビジョン (CV)**: 画像分類、物体検出、セグメンテーション。
  * **音声処理**: 音声認識、音声合成。
  * **マルチモーダル**: CLIP, GPT-4V などでテキスト＋画像＋音声の統合。
  * **科学・工学**: 蛋白質構造予測 (AlphaFold)、自動運転、HPC最適化。

## ニューラルネットワーク | Neural Network

- 人間の脳神経回路に着想を得て設計された非線形関数近似モデルで、複雑なパターン認識や関数近似を行う。
- 以下の式で表せる**層**を積み重ねた構造になっていて、ある層の出力が次の層への入力となる。

$$
h^{(l)} = f\left(W^{(l)} h^{(l-1)} + b^{(l)}\right)
$$

  * $h^{(l)}$: 層 $l$ の出力
  * $W^{(l)}$: 重み行列
  * $b^{(l)}$: バイアス
  * $f$: 活性化関数（ReLU, GELU, etc.）

- 従来の統計学的手法 単回帰、重回帰、線形分類モデルは、一つの層から構成されるニューラルネットワーク、すなわち単層ニューラルネットワーク（single-layer neural network）と見なすことができる。これらは、線形分離可能な問題しか扱えない。
- 一方で、ニューラルネットワークには万能近似能力がある。
  - 二層以上のニューラルネットワークで、
  - 活性化関数が非線形で、
  - 中間層(=1層目の出力 すなわち2層目の入力)の次元が無限で、
  - 訓練データが無限にあれば、
  - どんな関数でも近似できるというもの。
- (参考):
  - https://ibis.t.u-tokyo.ac.jp/suzuki/lecture/2020/intensive2/Kyusyu_2020_Deep.pdf
  -  https://chokkan.github.io/mlnote/classification/03nn.html



## 活性化関数 | 伝達関数 | Activation Function
- ニューラルネットワークを構成要素の1つ。
  - 典型的には、ニューラルネットワークは複数の層から構成されており、
  - ある層の出力が次の層への入力となっていて、
  - ある層の出力は、
    - ある層の入力xに対し**線形変換** (重み付き和 + バイアス) した後に、
    - **非線形関数**を通したものとして計算される。この非線形関数を活性化関数をと呼ぶ。
- 活性化関数には、ニューラルネットワークが線形分離可能でない問題を扱えるようにするという重要の役割がある(→詳細はニューラルネットワークの節を参照)。

* **具体例**:
  1. **Sigmoid**
     $\sigma(x)=\frac{1}{1+e^{-x}}$

     * 範囲 (0,1)。古典的。勾配消失が起きやすい。
  2. **tanh**
     $\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

     * 範囲 (-1,1)。
  3. **ReLU (Rectified Linear Unit)**
     $f(x)=\max(0,x)$
     * 現代の標準。計算効率が高い。
  4. **GELU (Gaussian Error Linear Unit)**
     $f(x)=x\cdot \Phi(x)$ （$\Phi$ は標準正規分布のCDF）
     * Transformer系モデル（BERT, GPT）で採用。ReLUより滑らか。
  5. **SiLU/Swish**
     $f(x)=x \cdot \sigma(x)$
     * GPT-4, PaLM などで利用。

## 重み | 重みパラメータ | Weight | Weight Parameter
- ニューラルネットワーク内部の**学習可能な係数（行列・ベクトル・バイアス）の総称**。
- 誤差逆伝播法で最適化される対象。

## ファインチューニング | Fine-tuning
- 既に学習済みモデルの重みを初期値として、別のタスクに適用させるべく追加でのデータセットで追加学習すること。
  - 画像分類モデルの専門領域適応(一般的な画像分類モデルを車両分類モデルに特化させる)
  - LLMの専門領域適応（医学、法律、金融など）
  - LLMの応答のスタイル調整（対話口調、企業ポリシー適合）
- 種類:
  - **フルファインチューニング:** 全重みパラメータを更新する。
  - **部分ファインチューニング|フリーズ:** 特定の重みパラメータだけを更新する
  - **PEFT:** アダプターと呼ばれる追加の重みパラメータを用意し、そのアダプターだけ学習する

## フリーズ | Freeze
- ファインチューニングの文脈では、部分ファインチューニングやPEFTにおいて、ファインチューニング対象の深層学習モデルの重みパラメータの一部をファインチューニングで更新しないように固定することを指して、フリーズと呼ぶ。
- 例えば「出力層以外の重みパラメータをフリーズする。」というように用いられる。

## 学習率 | Learning Rate
- 勾配降下法において、各ステップでパラメータをどれだけ更新するかを調整するハイパーパラメータ。
- 下記式で、学習率$\eta$が大きすぎると発散し、小さすぎると収束が遅くなる。
  - ※ 発散する・・・学習率が大きすぎると、最適解を飛び越えてしまい、損失関数が減少せずにむしろ増加してしまう現象。
  - ※ 収束する・・・学習が進むにつれて損失関数の値が徐々に減少し、最適解に近づいていくこと。

$$
\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)
$$
* $\eta$: 学習率
* $\nabla_\theta L(\theta)$: 勾配

## Optimizer | 最適化手法
- 損失関数の勾配を基に、モデルの重みパラメータを更新するアルゴリズム。様々なアルゴリズムが考案されている。
- 具体例:
  1. **SGD (Stochastic Gradient Descent)**
     * 最も基本。$\eta$は学習率。
       $$
       \theta \leftarrow \theta - \eta \nabla_\theta L(\theta)
       $$
  2. **Momentum SGD**
     * 勾配の移動平均を利用して収束を加速。
  3. **Adam (Adaptive Moment Estimation)**
     * \*\*1次モーメント（平均）と2次モーメント（分散）\*\*を組み合わせる。
     * LLMで最も一般的（特に AdamW = Adam + Weight Decay）。
  4. **AdaGrad, RMSProp, Lion** なども存在。


## Scheduler | 学習率スケジューラ
- 学習率 (Learning Rate) を **訓練ステップに応じて動的に変化させる仕組み**。様々な手法が考案されている。
- 具体例:
  * **Step Decay**: 一定ステップごとに学習率を減少。
  * **Exponential Decay**: 学習率を指数的に減少。
  * **Cosine Annealing**: 学習率をコサインカーブで減少。
  * **Warmup + Decay**: 初期は徐々に増加（Warmup）、その後減衰。

## Dropout | ドロップアウト
- ニューラルネットワークの学習中に、ある層の出力yの各要素について確率 $p$ でニューロンを無効化する正則化手法。
- 過学習 (overfitting) を防ぎ、汎化性能を向上する効果がある。
- 学習中だけ適用され、推論時にはドロップアウトは無効化される。

$$
h_i' = \begin{cases} 
  0 & \text{with prob } p \\ 
  \frac{h_i}{1-p} & \text{with prob } 1-p
\end{cases}
$$

## 自己教師あり学習 | Self-supervised Learning
- ラベルなしデータから部分的にタスクを構成し、教師あり学習のように学習する手法。

## 勾配降下法 | Gradient Descent
- 誤差関数の勾配を計算し、負の方向へパラメータを更新する最適化アルゴリズム。バッチサイズに応じてSGD、Mini-batch、Full-batchなどに分かれ、変種としてAdamやRMSpropがある。

## プーリング層 | Poolingレイヤー | Pooling Layer
- 畳み込み後の特徴マップを空間的にダウンサンプリングする操作。MaxPoolingやAveragePoolingが主流。

## エポック | Epoch
- 訓練データ全体を1回通して学習するサイクル。過学習回避のため早期終了と併用。

## ハイパーパラメータ | Hyperparameter
- 訓練前に設定する調整変数で、バッチサイズ、学習率、ドロップアウト率などが含まれる。

## バッチサイズ | Batch Size
- 1回のパラメータ更新に使うサンプル数。メモリ使用と学習安定性に影響。

## リカレントニューラルネットワーク | RNN
- 系列データに適したネットワークで、隠れ状態を時間的に伝播。勾配消失の影響を受けやすい。

## 勾配消失問題 | Vanishing Gradient Problem
- 活性化関数や深い層の影響で勾配が消失し、学習が進行しなくなる現象。ReLUや残差接続で緩和可能。

## 変分オートエンコーダー | VAE
- 潜在変数モデルをニューラルネットワークで構成し、ELBOの最小化により訓練される生成モデル。

## 拡散モデル | Diffusion Model
- ノイズ付加と復元の2過程でデータ生成を行う確率的生成モデル。DDPMやStable Diffusionが代表。

## 損失関数 | Loss Function
- モデル出力と教師信号との誤差を数値化する指標で、勾配降下法の最小化対象。代表例にクロスエントロピーやMSE。

## 教師あり学習 | Supervised Learning
- 入力と正解ラベルの対を用いた学習手法。分類や回帰が該当。

## 敵対的生成ネットワーク | GAN
- 生成器と識別器がミニマックスゲームを行う構造を持つ生成モデル。潜在空間から高品質サンプル生成が可能。

## 物体検出 | Object Detection
- 画像内の対象物を局所化（バウンディングボックス）し、分類も行う複合タスク。YOLOやFaster R-CNNが代表。

## 畳み込みニューラルネットワーク | CNN
- 空間的局所性とパラメータ共有を活用し、画像や時系列データの処理に特化したディープネットワーク。

## 誤差逆伝播法 | バックプロパゲーション | Backpropagation
- チェインルールにより勾配を層ごとに計算し、重み更新に利用。

## 転移学習 | Transfer Learning
- 事前学習済みモデルの知識を別タスクに応用する学習パラダイム。ファインチューニングと併用。

## 過学習 | Overfitting
- 訓練誤差が小さいが汎化誤差が大きい状態。正則化やデータ拡張で緩和可能。

## 音声認識 | Automatic Speech Recognition | ASR
- 音響特徴量から系列ラベル（文字列）への変換問題。典型的にはCTC損失やRNN-Tを使用。

## コサイン類似度 | Cosine Similarity
- 2つのベクトルがどれだけ同じ方向を向いているかを数値で表す指標。
- 具体的には、2つのベクトル $u, v$ のコサイン類似度とは、 $u, v$のなす角の余弦(下記式)。
  - 方向の近さを評価する。ベクトルの大きさは考慮しない。
  $$
  \cos(u,v) = \frac{u \cdot v}{\|u\|\|v\|}
  $$

- TF-IDFやword2vecなどの手法である文章をベクトル化した結果(=Embedding)の比較に用いられる。
