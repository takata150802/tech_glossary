<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md# -->

## 深層学習 | Deep Learning
- 多層のニューラルネットワークを用いて**データの階層的特徴表現を学習**する機械学習の一分野。
- 人手で特徴量設計する手法に対し、深層学習は**特徴量抽出から予測までをエンドツーエンドで最適化**する。

- 主なモデル:
  * **DNN (Deep Neural Network)**: 全結合層の多層ネットワーク。
  * **CNN (Convolutional Neural Network)**: 画像認識、ResNet, EfficientNet。
  * **RNN (Recurrent Neural Network)**: 系列データ、LSTM, GRU。
  * **Transformer**: 自己注意機構による並列処理。GPT, BERT, ViT。
- 応用分野:
  * **自然言語処理 (NLP)**: 機械翻訳、要約、対話（LLM）。
  * **コンピュータビジョン (CV)**: 画像分類、物体検出、セグメンテーション。
  * **音声処理**: 音声認識、音声合成。
  * **マルチモーダル**: CLIP, GPT-4V などでテキスト＋画像＋音声の統合。
  * **科学・工学**: 蛋白質構造予測 (AlphaFold)、自動運転、HPC最適化。

## ニューラルネットワーク | Neural Network

- 人間の脳神経回路に着想を得て設計された非線形関数近似モデルで、複雑なパターン認識や関数近似を行う。
- 以下の式で表せる**層**を積み重ねた構造になっていて、ある層の出力が次の層への入力となる。

  $$
  h^{(l)} = f\left(W^{(l)} h^{(l-1)} + b^{(l)}\right)
  $$

  * $h^{(l)}$: 層 $l$ の出力
  * $W^{(l)}$: 重み行列
  * $b^{(l)}$: バイアス
  * $f$: 活性化関数（ReLU, GELU, etc.）

- 従来の統計学的手法 単回帰、重回帰、線形分類モデルは、一つの層から構成されるニューラルネットワーク、すなわち単層ニューラルネットワーク（single-layer neural network）と見なすことができる。これらは、線形分離可能な問題しか扱えない。
- 一方で、ニューラルネットワークには万能近似能力がある。
  - 二層以上のニューラルネットワークで、
  - 活性化関数が非線形で、
  - 中間層(=1層目の出力 すなわち2層目の入力)の次元が無限で、
  - 訓練データが無限にあれば、
  - どんな関数でも近似できるというもの。
- (参考):
  - https://ibis.t.u-tokyo.ac.jp/suzuki/lecture/2020/intensive2/Kyusyu_2020_Deep.pdf
  -  https://chokkan.github.io/mlnote/classification/03nn.html



## 活性化関数 | 伝達関数 | Activation Function
- ニューラルネットワークを構成要素の1つ。
  - 典型的には、ニューラルネットワークは複数の層から構成されており、
  - ある層の出力が次の層への入力となっていて、
  - ある層の出力は、
    - ある層の入力xに対し**線形変換** (重み付き和 + バイアス) した後に、
    - **非線形関数**を通したものとして計算される。この非線形関数を活性化関数をと呼ぶ。
- 活性化関数には、ニューラルネットワークが線形分離可能でない問題を扱えるようにするという重要の役割がある(→詳細はニューラルネットワークの節を参照)。

* **具体例**:
  1. **Sigmoid**
     $\sigma(x)=\frac{1}{1+e^{-x}}$

     * 範囲 (0,1)。古典的。勾配消失が起きやすい。
  2. **tanh**
     $\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

     * 範囲 (-1,1)。RNNで利用。
  3. **ReLU (Rectified Linear Unit)**
     $f(x)=\max(0,x)$

     * 現代の標準。計算効率が高い。
     * 問題: **死んだReLU**（勾配ゼロ化）。
  4. **GELU (Gaussian Error Linear Unit)**
     $f(x)=x\cdot \Phi(x)$ （$\Phi$ は標準正規分布のCDF）

     * Transformer系モデル（BERT, GPT）で採用。ReLUより滑らか。
  5. **SiLU/Swish**
     $f(x)=x \cdot \sigma(x)$

     * GPT-4, PaLM などで利用。

## 重み | 重みパラメータ | Weight | Weight Parameter
- モデル内部で学習される重みおよびバイアスで、誤差逆伝播法で最適化される対象。

## ファインチューニング | Fine-tuning
- 事前学習済みモデルに対し、特定のタスクやドメインに合わせて勾配更新を行うトレーニング。

## 学習率 | Learning Rate
- 勾配降下法において、各ステップでパラメータをどれだけ更新するかを制御するハイパーパラメータ。

## 自己教師あり学習 | Self-supervised Learning
- ラベルなしデータから部分的にタスクを構成し、教師あり学習のように学習する手法。

## 勾配降下法 | Gradient Descent
- 誤差関数の勾配を計算し、負の方向へパラメータを更新する最適化アルゴリズム。バッチサイズに応じてSGD、Mini-batch、Full-batchなどに分かれ、変種としてAdamやRMSpropがある。

## プーリング層 | Poolingレイヤー | Pooling Layer
- 畳み込み後の特徴マップを空間的にダウンサンプリングする操作。MaxPoolingやAveragePoolingが主流。

## エポック | Epoch
- 訓練データ全体を1回通して学習するサイクル。過学習回避のため早期終了と併用。

## ドロップアウト | Dropout
- ニューラルネットワークの過学習防止のために、学習時ランダムにユニットを無効化する正則化手法。

## ハイパーパラメータ | Hyperparameter
- 訓練前に設定する調整変数で、バッチサイズ、学習率、ドロップアウト率などが含まれる。

## バッチサイズ | Batch Size
- 1回のパラメータ更新に使うサンプル数。メモリ使用と学習安定性に影響。

## リカレントニューラルネットワーク | RNN
- 系列データに適したネットワークで、隠れ状態を時間的に伝播。勾配消失の影響を受けやすい。

## 勾配消失問題 | Vanishing Gradient Problem
- 活性化関数や深い層の影響で勾配が消失し、学習が進行しなくなる現象。ReLUや残差接続で緩和可能。

## 変分オートエンコーダー | VAE
- 潜在変数モデルをニューラルネットワークで構成し、ELBOの最小化により訓練される生成モデル。

## 拡散モデル | Diffusion Model
- ノイズ付加と復元の2過程でデータ生成を行う確率的生成モデル。DDPMやStable Diffusionが代表。

## 損失関数 | Loss Function
- モデル出力と教師信号との誤差を数値化する指標で、勾配降下法の最小化対象。代表例にクロスエントロピーやMSE。

## 教師あり学習 | Supervised Learning
- 入力と正解ラベルの対を用いた学習手法。分類や回帰が該当。

## 敵対的生成ネットワーク | GAN
- 生成器と識別器がミニマックスゲームを行う構造を持つ生成モデル。潜在空間から高品質サンプル生成が可能。

## 物体検出 | Object Detection
- 画像内の対象物を局所化（バウンディングボックス）し、分類も行う複合タスク。YOLOやFaster R-CNNが代表。

## 畳み込みニューラルネットワーク | CNN
- 空間的局所性とパラメータ共有を活用し、画像や時系列データの処理に特化したディープネットワーク。

## 誤差逆伝播法 | バックプロパゲーション | Backpropagation
- チェインルールにより勾配を層ごとに計算し、重み更新に利用。

## 転移学習 | Transfer Learning
- 事前学習済みモデルの知識を別タスクに応用する学習パラダイム。ファインチューニングと併用。

## 過学習 | Overfitting
- 訓練誤差が小さいが汎化誤差が大きい状態。正則化やデータ拡張で緩和可能。

## 音声認識 | Automatic Speech Recognition | ASR
- 音響特徴量から系列ラベル（文字列）への変換問題。典型的にはCTC損失やRNN-Tを使用。

## コサイン類似度 | Cosine Similarity
- 2つのベクトルがどれだけ同じ方向を向いているかを数値で表す指標。
- 具体的には、2つのベクトル $u, v$ のコサイン類似度とは、 $u, v$のなす角の余弦(下記式)。
  - 方向の近さを評価する。ベクトルの大きさは考慮しない。
  $$
  \cos(u,v) = \frac{u \cdot v}{\|u\|\|v\|}
  $$

- TF-IDFやword2vecなどの手法である文章をベクトル化した結果(=Embedding)の比較に用いられる。
