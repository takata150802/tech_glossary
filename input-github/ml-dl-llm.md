<!-- 記事タイトル:用語解説集-機械学習-深層学習-LLM -->
<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md# -->

## 大規模言語モデル | Large Language Model | LLM <!-- entry_word_and_anchor:ML_DL_LLM_LargeLangeageModel -->
- 膨大な量のテキストデータを使用してトレーニングされた深層学習ニューラルネットワーク。
- これらのモデルは、自然言語処理タスク（文章生成、機械翻訳、質問応答など）において非常に高い性能を発揮。
- 数億から数千億のパラメータを持ち、多くの場合、巨大なコンピューティングリソースと膨大なデータセットを使用してトレーニングされる。
- これにより、モデルは人間の言語理解能力に近いレベルのパフォーマンスを実現することができる。
- 具体例:
  - GPT-3 
  - GPT-2
  - BERT
  - T5
  - XLNet
  - RoBERTa



## GPT-3 | Generative Pre-trained Transformer 3 <!-- entry_word_and_anchor:ML_DL_LLM_GPT3 -->
- OpenAIによって開発された、数千億のパラメータを持つ言語モデル。

## GPT-2 | Generative Pre-trained Transformer 2 <!-- entry_word_and_anchor:ML_DL_LLM_GPT2 -->
- OpenAIが開発した、数億のパラメータを持つ言語モデルの前身。


## BERT | Bidirectional Encoder Representations from Transformers <!-- entry_word_and_anchor:ML_DL_LLM_BERT -->
- Googleが提案した、大規模な事前学習済み言語モデル。

## T5 | Text-To-Text Transfer Transformer <!-- entry_word_and_anchor:ML_DL_LLM_T5 -->
- Google Researchが提案した、様々な自然言語処理タスクを統一的なテキスト形式に変換して解決する手法。

## XLNet  <!-- entry_word_and_anchor:ML_DL_LLM_XLNet -->
- Google BrainチームとCMUによって開発された、文脈依存性を考慮した言語モデル。

## RoBERTa | Robustly optimized BERT approach <!-- entry_word_and_anchor:ML_DL_LLM_RoBERTa -->
- Facebook AI Researchが開発したBERTの改良版で、トレーニング手法やパラメータのチューニングを改善したモデル。
