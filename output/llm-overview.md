<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/llm-overview.md# -->

## 大規模言語モデル | Large Language Model | LLM<a id="5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrIHwgTGFyZ2UgTGFuZ3VhZ2UgTW9kZWwgfCBMTE0="></a>

- 膨大な量のテキストデータを使用してトレーニングされた<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw==">深層学習</a><a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>。
- これらのモデルは、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#6Ieq54S26KiA6Kqe5Yem55CGIHwgTmF0dXJhbCBMYW5ndWFnZSBQcm9jZXNzaW5nIHwgTkxQ">自然言語処理</a>タスク（文章生成、機械翻訳、質問応答など）において非常に高い性能を発揮。
- 数億から数千億のパラメータを持ち、多くの場合、巨大なコンピューティングリソースと膨大なデータセットを使用してトレーニングされる。
- これにより、モデルは人間の言語理解能力に近いレベルのパフォーマンスを実現することができる。
- 具体例:

## OpenAI<a id="T3BlbkFJ"></a>

## Hugging Face<a id="SHVnZ2luZyBGYWNl"></a>

## RAG | Retrieval Augmented Generation<a id="UkFHIHwgUmV0cmlldmFsIEF1Z21lbnRlZCBHZW5lcmF0aW9u"></a>
