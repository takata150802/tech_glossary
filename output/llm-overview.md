<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/llm-overview.md# -->

<a id="ML_LLM_LargeLangeageModel"></a>
## 大規模言語モデル | Large Language Model | LLM <!-- entry_word_and_anchor:ML_LLM_LargeLangeageModel -->
- 膨大な量のテキストデータを使用してトレーニングされた<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_DeepLearing_"> 深層学習 </a><a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_Neural_Network"> ニューラルネットワーク </a>。
- これらのモデルは、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_NLP"> 自然言語処理 </a>タスク（文章生成、機械翻訳、質問応答など）において非常に高い性能を発揮。
- 数億から数千億のパラメータを持ち、多くの場合、巨大なコンピューティングリソースと膨大なデータセットを使用してトレーニングされる。
- これにより、モデルは人間の言語理解能力に近いレベルのパフォーマンスを実現することができる。
- 具体例:

<a id="ML_LLM_OpenAI"></a>
## OpenAI <!-- entry_word_and_anchor:ML_LLM_OpenAI -->

<a id="ML_LLM_HuggingFace"></a>
## Hugging Face <!-- entry_word_and_anchor:ML_LLM_HuggingFace -->

<a id="ML_LLM_RAG"></a>
## RAG | Retrieval Augmented Generation | RAG <!-- entry_word_and_anchor:ML_LLM_RAG -->
