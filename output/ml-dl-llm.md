<!-- 記事タイトル:用語解説集-機械学習-深層学習-LLM -->
<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md# -->

<a id="ML_DL_LLM_LargeLangeageModel"></a>
## 大規模言語モデル | Large Language Model | LLM <!-- entry_word_and_anchor:ML_DL_LLM_LargeLangeageModel -->
- 膨大な量のテキストデータを使用してトレーニングされた<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_DeepLearing_"> 深層学習 </a><a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_Neural_Network"> ニューラルネットワーク </a>。
- これらのモデルは、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_NLP"> 自然言語処理 </a>タスク（文章生成、機械翻訳、質問応答など）において非常に高い性能を発揮。
- 数億から数千億のパラメータを持ち、多くの場合、巨大なコンピューティングリソースと膨大なデータセットを使用してトレーニングされる。
- これにより、モデルは人間の言語理解能力に近いレベルのパフォーマンスを実現することができる。
- 具体例:
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_GPT3"> GPT-3 </a> 
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_GPT2"> GPT-2 </a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_BERT"> BERT </a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_T5"> T5 </a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_XLNet"> XLNet </a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_RoBERTa"> RoBERTa </a>



<a id="ML_DL_LLM_GPT3"></a>
## GPT-3 | Generative Pre-trained Transformer 3 <!-- entry_word_and_anchor:ML_DL_LLM_GPT3 -->
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/llm-overview.md#ML_LLM_OpenAI"> OpenAI </a>によって開発された、数千億のパラメータを持つ言語モデル。

<a id="ML_DL_LLM_GPT2"></a>
## GPT-2 | Generative Pre-trained Transformer 2 <!-- entry_word_and_anchor:ML_DL_LLM_GPT2 -->
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/llm-overview.md#ML_LLM_OpenAI"> OpenAI </a>が開発した、数億のパラメータを持つ言語モデルの前身。


<a id="ML_DL_LLM_BERT"></a>
## BERT | Bidirectional Encoder Representations from Transformers <!-- entry_word_and_anchor:ML_DL_LLM_BERT -->
- Googleが提案した、大規模な事前学習済み言語モデル。

<a id="ML_DL_LLM_T5"></a>
## T5 | Text-To-Text Transfer Transformer <!-- entry_word_and_anchor:ML_DL_LLM_T5 -->
- Google Researchが提案した、様々な<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_NLP"> 自然言語処理 </a>タスクを統一的なテキスト形式に変換して解決する手法。

<a id="ML_DL_LLM_XLNet"></a>
## XLNet  <!-- entry_word_and_anchor:ML_DL_LLM_XLNet -->
- Google BrainチームとCMUによって開発された、文脈依存性を考慮した言語モデル。

<a id="ML_DL_LLM_RoBERTa"></a>
## RoBERTa | Robustly optimized BERT approach <!-- entry_word_and_anchor:ML_DL_LLM_RoBERTa -->
- Facebook AI Researchが開発した<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_BERT"> BERT </a>の改良版で、トレーニング手法やパラメータのチューニングを改善したモデル。
