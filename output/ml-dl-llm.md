<!-- 記事タイトル:用語解説集-機械学習-深層学習-LLM -->

<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md# -->

## 大規模言語モデル | Large Language Model | LLM<a id="5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrIHwgTGFyZ2UgTGFuZ3VhZ2UgTW9kZWwgfCBMTE0="></a>

- 膨大な量のテキストデータを使用してトレーニングされた<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw==">深層学習</a><a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>。
- これらのモデルは、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#6Ieq54S26KiA6Kqe5Yem55CGIHwgTmF0dXJhbCBMYW5ndWFnZSBQcm9jZXNzaW5nIHwgTkxQ">自然言語処理</a>タスク（文章生成、機械翻訳、質問応答など）において非常に高い性能を発揮。
- 数億から数千億のパラメータを持ち、多くの場合、巨大なコンピューティングリソースと膨大なデータセットを使用してトレーニングされる。
- これにより、モデルは人間の言語理解能力に近いレベルのパフォーマンスを実現することができる。
- 具体例:
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#R1BULTMgfCBHZW5lcmF0aXZlIFByZS10cmFpbmVkIFRyYW5zZm9ybWVyIDM=">GPT-3</a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#R1BULTIgfCBHZW5lcmF0aXZlIFByZS10cmFpbmVkIFRyYW5zZm9ybWVyIDI=">GPT-2</a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#QkVSVCB8IEJpZGlyZWN0aW9uYWwgRW5jb2RlciBSZXByZXNlbnRhdGlvbnMgZnJvbSBUcmFuc2Zvcm1lcnM=">BERT</a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#VDUgfCBUZXh0LVRvLVRleHQgVHJhbnNmZXIgVHJhbnNmb3JtZXI=">T5</a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#WExOZXQ=">XLNet</a>
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#Um9CRVJUYSB8IFJvYnVzdGx5IG9wdGltaXplZCBCRVJUIGFwcHJvYWNo">RoBERTa</a>

## GPT-3 | Generative Pre-trained Transformer 3<a id="R1BULTMgfCBHZW5lcmF0aXZlIFByZS10cmFpbmVkIFRyYW5zZm9ybWVyIDM="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/llm-overview.md#T3BlbkFJ">OpenAI</a>によって開発された、数千億のパラメータを持つ言語モデル。

## GPT-2 | Generative Pre-trained Transformer 2<a id="R1BULTIgfCBHZW5lcmF0aXZlIFByZS10cmFpbmVkIFRyYW5zZm9ybWVyIDI="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/llm-overview.md#T3BlbkFJ">OpenAI</a>が開発した、数億のパラメータを持つ言語モデルの前身。

## BERT | Bidirectional Encoder Representations from Transformers<a id="QkVSVCB8IEJpZGlyZWN0aW9uYWwgRW5jb2RlciBSZXByZXNlbnRhdGlvbnMgZnJvbSBUcmFuc2Zvcm1lcnM="></a>

- Googleが提案した、大規模な<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-training.md#5LqL5YmN5a2m57+SIHwgUHJlLXRyYWluaW5n">事前学習</a>済み言語モデル。

## T5 | Text-To-Text Transfer Transformer<a id="VDUgfCBUZXh0LVRvLVRleHQgVHJhbnNmZXIgVHJhbnNmb3JtZXI="></a>

- Google Researchが提案した、様々な<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#6Ieq54S26KiA6Kqe5Yem55CGIHwgTmF0dXJhbCBMYW5ndWFnZSBQcm9jZXNzaW5nIHwgTkxQ">自然言語処理</a>タスクを統一的なテキスト形式に変換して解決する手法。

## XLNet<a id="WExOZXQ="></a>

- Google BrainチームとCMUによって開発された、文脈依存性を考慮した言語モデル。

## RoBERTa | Robustly optimized BERT approach<a id="Um9CRVJUYSB8IFJvYnVzdGx5IG9wdGltaXplZCBCRVJUIGFwcHJvYWNo"></a>

- Facebook <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a> Researchが開発した<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#QkVSVCB8IEJpZGlyZWN0aW9uYWwgRW5jb2RlciBSZXByZXNlbnRhdGlvbnMgZnJvbSBUcmFuc2Zvcm1lcnM=">BERT</a>の改良版で、トレーニング手法やパラメータのチューニングを改善したモデル。
