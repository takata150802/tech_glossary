<!-- 記事タイトル:用語解説集-機械学習-深層学習-言語系 -->
<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md# -->

<a id="ML_DL_NLP_Transformer"></a>
## Transformer  <!-- entry_word_and_anchor:ML_DL_NLP_Transformer -->

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_NLP"> NLP </a>"> 自然言語処理 </a>タスクにおける革新的なモデルアーキテクチャであり、従来の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_RNN"> リカレント<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_Neural_Network"> ニューラルネットワーク </a> </a>（<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_RNN"> RNN </a>）や<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_CNN"> 畳み込みニューラルネットワーク </a>（<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_CNN"> CNN </a>）に代わるものとして注目された。
- シーケンスからシーケンスへの変換（例: 機械翻訳）、文章の生成、質問応答などのさまざまな<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_NLP"> 自然言語処理 </a>タスクで成功を収めました。

<a id="ML_DL_NLP_AttentionIsAllYouNeed"></a>
## Attention Is All You Need <!-- entry_word_and_anchor:ML_DL_NLP_AttentionIsAllYouNeed -->
- 2017年にGoogleの研究者によって提案された論文。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_Transformer"> <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md#ML_DL_<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_NLP"> NLP </a>_Transformer"> Transformer </a> </a>と呼ばれる<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_Neural_Network"> ニューラルネットワーク </a>アーキテクチャを導入した。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/llm-overview.md#ML_LLM_LargeLangeageModel"> LLM </a>

<a id="ML_DL_NLP_LSTM"></a>
## LSTM | Long Short-Term Memory <!-- entry_word_and_anchor:ML_DL_NLP_LSTM -->
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_RNN"> リカレントニューラルネットワーク </a>（<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#ML_DL_RNN"> RNN </a>）の一種。
- 長期的な依存関係を学習するのに適したアーキテクチャ。
- 通常、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#ML_ML_OVERVIEW_NLP"> 自然言語処理 </a>や時系列データなどのシーケンスデータのモデリングに使用される。

<a id="ML_DL_NLP_Tokenization"></a>
## トークン化 | Tokenization  <!-- entry_word_and_anchor:ML_DL_NLP_Tokenization -->
- ある文章を、単語、句読点、数字、記号などに分割すること。
- 文章を数理モデルで扱える形式に変換するために行う。

<a id="ML_DL_NLP_Embedding"></a>
## Embedding <!-- entry_word_and_anchor:ML_DL_NLP_Embedding -->
- トークン(<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md#ML_DL_NLP_Tokenization"> トークン化 </a>参照)を実数のベクトルに変換すること。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md#ML_DL_NLP_Tokenization"> トークン化 </a>と同様、文章を数理モデルで扱える形式に変換するために行う。
