<!-- 記事タイトル:用語解説集-機械学習-深層学習-言語系 -->
<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md# -->

<a id="ML_DL_NLP_Transformer"></a>
## Transformer  <!-- entry_word_and_anchor:ML_DL_NLP_Transformer -->

- 自然言語処理タスクにおける革新的なモデルアーキテクチャであり、従来のリカレントニューラルネットワーク（RNN）や畳み込みニューラルネットワーク（CNN）に代わるものとして注目された。
- シーケンスからシーケンスへの変換（例: 機械翻訳）、文章の生成、質問応答などのさまざまな自然言語処理タスクで成功を収めました。

<a id="ML_DL_NLP_AttentionIsAllYouNeed"></a>
## Attention Is All You Need <!-- entry_word_and_anchor:ML_DL_NLP_AttentionIsAllYouNeed -->
- 2017年にGoogleの研究者によって提案された論文。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md#ML_DL_NLP_Transformer"> Transformer </a>と呼ばれるニューラルネットワークアーキテクチャを導入した。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#ML_DL_LLM_LargeLangeageModel"> LLM </a>

<a id="ML_DL_NLP_LSTM"></a>
## LSTM | Long Short-Term Memory <!-- entry_word_and_anchor:ML_DL_NLP_LSTM -->
- リカレントニューラルネットワーク（RNN）の一種。
- 長期的な依存関係を学習するのに適したアーキテクチャ。
- 通常、自然言語処理や時系列データなどのシーケンスデータのモデリングに使用される。

<a id="ML_DL_NLP_Tokenization"></a>
## トークン化 | Tokenization  <!-- entry_word_and_anchor:ML_DL_NLP_Tokenization -->
- ある文章を、単語、句読点、数字、記号などに分割すること。
- 文章を数理モデルで扱える形式に変換するために行う。

<a id="ML_DL_NLP_Embedding"></a>
## Embedding <!-- entry_word_and_anchor:ML_DL_NLP_Embedding -->
- トークン(<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md#ML_DL_NLP_Tokenization"> トークン化 </a>参照)を実数のベクトルに変換すること。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md#ML_DL_NLP_Tokenization"> トークン化 </a>と同様、文章を数理モデルで扱える形式に変換するために行う。
