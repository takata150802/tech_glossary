<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md# -->

## 深層学習 | Deep Learning<a id="5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw=="></a>

- 多層の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>を用いて**データの階層的特徴表現を学習**する<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5qmf5qKw5a2m57+SIHwgTWFjaGluZSBMZWFybmluZw==">機械学習</a>の一分野。

- 人手で<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>設計する手法に対し、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw==">深層学習</a>は**<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>抽出から予測までをエンドツーエンドで最適化**する。

- 主なモデル:

  - **DNN (Deep <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">Neural Network</a>)**: 全結合層の多層ネットワーク。
  - **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#55Wz44G/6L6844G/44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgQ05O">CNN</a> (Convolutional <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">Neural Network</a>)**: 画像認識、ResNet, EfficientNet。
  - **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44Oq44Kr44Os44Oz44OI44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgUk5O">RNN</a> (Recurrent <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">Neural Network</a>)**: 系列データ、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-nlp.md#TFNUTSB8IExvbmcgU2hvcnQtVGVybSBNZW1vcnk=">LSTM</a>, GRU。
  - **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#44OI44Op44Oz44K544OV44Kp44O844Oe44O8IHwgVHJhbnNmb3JtZXI=">Transformer</a>**: 自己注意機構による並列処理。GPT, <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#QkVSVCB8IEJpZGlyZWN0aW9uYWwgRW5jb2RlciBSZXByZXNlbnRhdGlvbnMgZnJvbSBUcmFuc2Zvcm1lcnM=">BERT</a>, ViT。

- 応用分野:

  - **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#6Ieq54S26KiA6Kqe5Yem55CGIHwgTkxQIHwgTmF0dXJhbCBMYW5ndWFnZSBQcm9jZXNzaW5n">自然言語処理</a> (<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#6Ieq54S26KiA6Kqe5Yem55CGIHwgTkxQIHwgTmF0dXJhbCBMYW5ndWFnZSBQcm9jZXNzaW5n">NLP</a>)**: 機械翻訳、要約、対話（<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>）。
  - **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#44Kz44Oz44OU44Ol44O844K/44OT44K444On44OzIHwgQ29tcHV0ZXIgVmlzaW9uIHwgQ1Y=">コンピュータビジョン</a> (<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#44Kz44Oz44OU44Ol44O844K/44OT44K444On44OzIHwgQ29tcHV0ZXIgVmlzaW9uIHwgQ1Y=">CV</a>)**: 画像<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#54mp5L2T5qSc5Ye6IHwgT2JqZWN0IERldGVjdGlvbg==">物体検出</a>、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#44K744Kw44Oh44Oz44OG44O844K344On44OzIHwgU2VnbWVudGF0aW9u">セグメンテーション</a>。
  - **音声処理**: <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6Z+z5aOw6KqN6K2YIHwgQXV0b21hdGljIFNwZWVjaCBSZWNvZ25pdGlvbiB8IEFTUg==">音声認識</a>、音声合成。
  - **マルチモーダル**: CLIP, GPT-4V などでテキスト＋画像＋音声の統合。
  - **科学・工学**: 蛋白質構造予測 (AlphaFold)、自動運転、HPC最適化。

## ニューラルネットワーク | Neural Network<a id="44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms="></a>

- 人間の脳神経回路に着想を得て設計された非線形関数近似モデルで、複雑なパターン認識や関数近似を行う。
- 以下の式で表せる**層**を積み重ねた構造になっていて、ある層の出力が次の層への入力となる。

$$
h^{(l)} = f\\left(W^{(l)} h^{(l-1)} + b^{(l)}\\right)
$$

- $h^{(l)}$: 層 $l$ の出力
- $W^{(l)}$: <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重み</a>行列
- $b^{(l)}$: <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44OQ44Kk44Ki44K5IHwgQmlhcw==">バイアス</a>
- $f$: <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rS75oCn5YyW6Zai5pWwIHwg5Lyd6YGU6Zai5pWwIHwgQWN0aXZhdGlvbiBGdW5jdGlvbg==">活性化関数</a>（ReLU, GELU, etc.）

* 従来の統計学的手法 単<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Zue5biwIHwgUmVncmVzc2lvbg==">回帰</a>、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6YeN5Zue5biwIHwgTXVsdGlwbGUgUmVncmVzc2lvbg==">重回帰</a>、線形<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>モデルは、一つの層から構成される<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>、すなわち単層<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>（single-layer neural network）と見なすことができる。これらは、線形分離可能な問題しか扱えない。
* 一方で、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>には万能近似能力がある。
  - 二層以上の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>で、
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rS75oCn5YyW6Zai5pWwIHwg5Lyd6YGU6Zai5pWwIHwgQWN0aXZhdGlvbiBGdW5jdGlvbg==">活性化関数</a>が非線形で、
  - 中間層(=1層目の出力 すなわち2層目の入力)の次元が無限で、
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6KiT57e044OH44O844K/IHwgVHJhaW5pbmcgRGF0YQ==">訓練データ</a>が無限にあれば、
  - どんな関数でも近似できるというもの。
* (参考):
  - https://ibis.t.u-tokyo.ac.jp/suzuki/lecture/2020/intensive2/Kyusyu_2020_Deep.pdf
  - https://chokkan.github.io/mlnote/classification/03nn.html

## 活性化関数 | 伝達関数 | Activation Function<a id="5rS75oCn5YyW6Zai5pWwIHwg5Lyd6YGU6Zai5pWwIHwgQWN0aXZhdGlvbiBGdW5jdGlvbg=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>を構成要素の1つ。
  - 典型的には、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>は複数の層から構成されており、
  - ある層の出力が次の層への入力となっていて、
  - ある層の出力は、
    - ある層の入力xに対し**線形変換** (<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重み</a>付き和 + <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44OQ44Kk44Ki44K5IHwgQmlhcw==">バイアス</a>) した後に、
    - **非線形関数**を通したものとして計算される。この非線形関数を<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rS75oCn5YyW6Zai5pWwIHwg5Lyd6YGU6Zai5pWwIHwgQWN0aXZhdGlvbiBGdW5jdGlvbg==">活性化関数</a>をと呼ぶ。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rS75oCn5YyW6Zai5pWwIHwg5Lyd6YGU6Zai5pWwIHwgQWN0aXZhdGlvbiBGdW5jdGlvbg==">活性化関数</a>には、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>が線形分離可能でない問題を扱えるようにするという重要の役割がある(→詳細は<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>の節を参照)。

* **具体例**:
  1. **Sigmoid**
     $\\sigma(x)=\\frac{1}{1+e^{-x}}$

     - 範囲 (0,1)。古典的。勾配消失が起きやすい。

  1. **tanh**
     $\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$

     - 範囲 (-1,1)。

  1. **ReLU (Rectified Linear Unit)**
     $f(x)=\\max(0,x)$

     - 現代の標準。計算効率が高い。

  1. **GELU (Gaussian Error Linear Unit)**
     $f(x)=x\\cdot \\Phi(x)$ （$\\Phi$ は標準正規分布のCDF）

     - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#44OI44Op44Oz44K544OV44Kp44O844Oe44O8IHwgVHJhbnNmb3JtZXI=">Transformer</a>系モデル（<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-dl-llm.md#QkVSVCB8IEJpZGlyZWN0aW9uYWwgRW5jb2RlciBSZXByZXNlbnRhdGlvbnMgZnJvbSBUcmFuc2Zvcm1lcnM=">BERT</a>, GPT）で採用。ReLUより滑らか。

  1. **SiLU/Swish**
     $f(x)=x \\cdot \\sigma(x)$

     - GPT-4, PaLM などで利用。

## 重み | 重みパラメータ | Weight | Weight Parameter<a id="6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>内部の**学習可能な係数（行列・ベクトル・<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44OQ44Kk44Ki44K5IHwgQmlhcw==">バイアス</a>）の総称**。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6Kqk5beu6YCG5Lyd5pKt5rOVIHwg44OQ44OD44Kv44OX44Ot44OR44Ky44O844K344On44OzIHwgQmFja3Byb3BhZ2F0aW9u">誤差逆伝播法</a>で最適化される対象。

## ファインチューニング | Fine-tuning<a id="44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc="></a>

- 既に学習済みモデルの<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重み</a>を初期値として、別のタスクに適用させるべく追加でのデータセットで追加学習すること。
  - 画像<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>モデルの専門領域適応(一般的な画像<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>モデルを車両<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>モデルに特化させる)
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>の専門領域適応（医学、法律、金融など）
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>の応答のスタイル調整（対話口調、企業ポリシー適合）
- 種類:
  - **フル<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc=">ファインチューニング</a>:** 全<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重みパラメータ</a>を更新する。
  - **部分<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc=">ファインチューニング</a>|<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Oq44O844K6IHwgRnJlZXpl">フリーズ</a>:** 特定の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重みパラメータ</a>だけを更新する
  - **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-training.md#UEVGVCB8IFBhcmFtZXRlci1FZmZpY2llbnQgRmluZS1UdW5pbmc=">PEFT</a>:** アダプターと呼ばれる追加の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重みパラメータ</a>を用意し、そのアダプターだけ学習する

## フリーズ | Freeze<a id="44OV44Oq44O844K6IHwgRnJlZXpl"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc=">ファインチューニング</a>の文脈では、部分<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc=">ファインチューニング</a>や<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-training.md#UEVGVCB8IFBhcmFtZXRlci1FZmZpY2llbnQgRmluZS1UdW5pbmc=">PEFT</a>において、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc=">ファインチューニング</a>対象の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw==">深層学習</a>モデルの<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重みパラメータ</a>の一部を<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc=">ファインチューニング</a>で更新しないように固定することを指して、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Oq44O844K6IHwgRnJlZXpl">フリーズ</a>と呼ぶ。
- 例えば「出力層以外の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重みパラメータ</a>を<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Oq44O844K6IHwgRnJlZXpl">フリーズ</a>する。」というように用いられる。

## 学習率 | Learning Rate<a id="5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5Yu+6YWN6ZmN5LiL5rOVIHwgR3JhZGllbnQgRGVzY2VudA==">勾配降下法</a>において、各ステップでパラメータをどれだけ更新するかを調整する<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OP44Kk44OR44O844OR44Op44Oh44O844K/IHwgSHlwZXJwYXJhbWV0ZXI=">ハイパーパラメータ</a>。
- 下記式で、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a>$\\eta$が大きすぎると発散し、小さすぎると収束が遅くなる。
  - ※ 発散する・・・<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a>が大きすぎると、最適解を飛び越えてしまい、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pCN5aSx6Zai5pWwIHwgTG9zcyBGdW5jdGlvbg==">損失関数</a>が減少せずにむしろ増加してしまう現象。
  - ※ 収束する・・・学習が進むにつれて<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pCN5aSx6Zai5pWwIHwgTG9zcyBGdW5jdGlvbg==">損失関数</a>の値が徐々に減少し、最適解に近づいていくこと。

$$
\\theta \\leftarrow \\theta - \\eta \\nabla\_\\theta L(\\theta)
$$

- $\\eta$: <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a>
- $\\nabla\_\\theta L(\\theta)$: 勾配

## Optimizer | 最適化手法<a id="T3B0aW1pemVyIHwg5pyA6YGp5YyW5omL5rOV"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pCN5aSx6Zai5pWwIHwgTG9zcyBGdW5jdGlvbg==">損失関数</a>の勾配を基に、モデルの<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重みパラメータ</a>を更新するアルゴリズム。様々なアルゴリズムが考案されている。
- 具体例:
  1. **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#56K6546H55qE5Yu+6YWN6ZmN5LiL5rOVIHwgU0dE">SGD</a> (Stochastic <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5Yu+6YWN6ZmN5LiL5rOVIHwgR3JhZGllbnQgRGVzY2VudA==">Gradient Descent</a>)**
     - 最も基本。$\\eta$は<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a>。
       $$
       \\theta \\leftarrow \\theta - \\eta \\nabla\_\\theta L(\\theta)
       $$
  1. **Momentum <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#56K6546H55qE5Yu+6YWN6ZmN5LiL5rOVIHwgU0dE">SGD</a>**
     - 勾配の移動平均を利用して収束を加速。
  1. **Adam (Adaptive Moment Estimation)**
     - \*\*1次モーメント（平均）と2次モーメント（分散）\*\*を組み合わせる。
     - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>で最も一般的（特に AdamW = Adam + <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">Weight</a> Decay）。
  1. **AdaGrad, RMSProp, Lion** なども存在。

## Scheduler | 学習率スケジューラ<a id="U2NoZWR1bGVyIHwg5a2m57+S546H44K544Kx44K444Ol44O844Op"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a> (<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">Learning Rate</a>) を **訓練ステップに応じて動的に変化させる仕組み**。様々な手法が考案されている。
- 具体例:
  - **Step Decay**: 一定ステップごとに<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a>を減少。
  - **Exponential Decay**: <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a>を指数的に減少。
  - **Cosine Annealing**: <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a>をコサインカーブで減少。
  - **Warmup + Decay**: 初期は徐々に増加（Warmup）、その後減衰。

## Dropout | ドロップアウト<a id="RHJvcG91dCB8IOODieODreODg+ODl+OCouOCpuODiA=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>の学習中に、ある層の出力yの各要素について確率 $p$ でニューロンを無効化する正則化手法。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YGO5a2m57+SIHwgT3ZlcmZpdHRpbmc=">過学習</a> (overfitting) を防ぎ、汎化性能を向上する効果がある。
- 学習中だけ適用され、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5o6o6KuWIHwgSW5mZXJlbmNl">推論</a>時には<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#RHJvcG91dCB8IOODieODreODg+ODl+OCouOCpuODiA==">ドロップアウト</a>は無効化される。

$$
h_i' = \\begin{cases}
0 & \\text{with prob } p \\
\\frac{h_i}{1-p} & \\text{with prob } 1-p
\\end{cases}
$$

## 自己教師あり学習 | Self-supervised Learning<a id="6Ieq5bex5pWZ5bir44GC44KK5a2m57+SIHwgU2VsZi1zdXBlcnZpc2VkIExlYXJuaW5n"></a>

- ラベルなしデータから部分的にタスクを構成し、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pWZ5bir44GC44KK5a2m57+SIHwgU3VwZXJ2aXNlZCBMZWFybmluZw==">教師あり学習</a>のように学習する手法。

## 勾配降下法 | Gradient Descent<a id="5Yu+6YWN6ZmN5LiL5rOVIHwgR3JhZGllbnQgRGVzY2VudA=="></a>

- 誤差関数の勾配を計算し、負の方向へパラメータを更新する最適化アルゴリズム。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OQ44OD44OB44K144Kk44K6IHwgQmF0Y2ggU2l6ZQ==">バッチサイズ</a>に応じて<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#56K6546H55qE5Yu+6YWN6ZmN5LiL5rOVIHwgU0dE">SGD</a>、Mini-batch、Full-batchなどに分かれ、変種としてAdamやRMSpropがある。

## プーリング層 | Poolingレイヤー | Pooling Layer<a id="44OX44O844Oq44Oz44Kw5bGkIHwgUG9vbGluZ+ODrOOCpOODpOODvCB8IFBvb2xpbmcgTGF5ZXI="></a>

- 畳み込み後の特徴マップを空間的にダウンサンプリングする操作。MaxPoolingやAveragePoolingが主流。

## エポック | Epoch<a id="44Ko44Od44OD44KvIHwgRXBvY2g="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6KiT57e044OH44O844K/IHwgVHJhaW5pbmcgRGF0YQ==">訓練データ</a>全体を1回通して学習するサイクル。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YGO5a2m57+SIHwgT3ZlcmZpdHRpbmc=">過学習</a>回避のため早期終了と併用。

## バッチサイズ | Batch Size<a id="44OQ44OD44OB44K144Kk44K6IHwgQmF0Y2ggU2l6ZQ=="></a>

- 1回のパラメータ更新に使うサンプル数。メモリ使用と学習安定性に影響。

## リカレントニューラルネットワーク | RNN<a id="44Oq44Kr44Os44Oz44OI44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgUk5O"></a>

- 系列データに適したネットワークで、隠れ状態を時間的に伝播。勾配消失の影響を受けやすい。

## 勾配消失問題 | Vanishing Gradient Problem<a id="5Yu+6YWN5raI5aSx5ZWP6aGMIHwgVmFuaXNoaW5nIEdyYWRpZW50IFByb2JsZW0="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rS75oCn5YyW6Zai5pWwIHwg5Lyd6YGU6Zai5pWwIHwgQWN0aXZhdGlvbiBGdW5jdGlvbg==">活性化関数</a>や深い層の影響で勾配が消失し、学習が進行しなくなる現象。ReLUや残差接続で緩和可能。

## 変分オートエンコーダー | VAE<a id="5aSJ5YiG44Kq44O844OI44Ko44Oz44Kz44O844OA44O8IHwgVkFF"></a>

- 潜在変数モデルを<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>で構成し、ELBOの最小化により訓練される生成モデル。

## 拡散モデル | Diffusion Model<a id="5ouh5pWj44Oi44OH44OrIHwgRGlmZnVzaW9uIE1vZGVs"></a>

- ノイズ付加と復元の2過程でデータ生成を行う確率的生成モデル。DDPMやStable Diffusionが代表。

## 損失関数 | Loss Function<a id="5pCN5aSx6Zai5pWwIHwgTG9zcyBGdW5jdGlvbg=="></a>

- モデル出力と教師信号との誤差を数値化する指標で、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5Yu+6YWN6ZmN5LiL5rOVIHwgR3JhZGllbnQgRGVzY2VudA==">勾配降下法</a>の最小化対象。代表例にクロスエントロピーやMSE。

## 教師あり学習 | Supervised Learning<a id="5pWZ5bir44GC44KK5a2m57+SIHwgU3VwZXJ2aXNlZCBMZWFybmluZw=="></a>

- 入力と正解ラベルの対を用いた学習手法。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>や<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Zue5biwIHwgUmVncmVzc2lvbg==">回帰</a>が該当。

## 敵対的生成ネットワーク | GAN<a id="5pW15a++55qE55Sf5oiQ44ON44OD44OI44Ov44O844KvIHwgR0FO"></a>

- 生成器と識別器がミニマックスゲームを行う構造を持つ生成モデル。潜在空間から高品質サンプル生成が可能。

## 物体検出 | Object Detection<a id="54mp5L2T5qSc5Ye6IHwgT2JqZWN0IERldGVjdGlvbg=="></a>

- 画像内の対象物を局所化（バウンディングボックス）し、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>も行う複合タスク。YOLOやFaster R-<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#55Wz44G/6L6844G/44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgQ05O">CNN</a>が代表。

## 畳み込みニューラルネットワーク | CNN<a id="55Wz44G/6L6844G/44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgQ05O"></a>

- 空間的局所性とパラメータ共有を活用し、画像や時系列データの処理に特化したディープネットワーク。

## 誤差逆伝播法 | バックプロパゲーション | Backpropagation<a id="6Kqk5beu6YCG5Lyd5pKt5rOVIHwg44OQ44OD44Kv44OX44Ot44OR44Ky44O844K344On44OzIHwgQmFja3Byb3BhZ2F0aW9u"></a>

- チェインルールにより勾配を層ごとに計算し、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重み</a>更新に利用。

## 転移学習 | Transfer Learning<a id="6Lui56e75a2m57+SIHwgVHJhbnNmZXIgTGVhcm5pbmc="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-training.md#5LqL5YmN5a2m57+SIHwgUHJlLXRyYWluaW5n">事前学習</a>済みモデルの知識を別タスクに応用する学習パラダイム。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OV44Kh44Kk44Oz44OB44Ol44O844OL44Oz44KwIHwgRmluZS10dW5pbmc=">ファインチューニング</a>と併用。

## 過学習 | Overfitting<a id="6YGO5a2m57+SIHwgT3ZlcmZpdHRpbmc="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6KiT57e06Kqk5beuIHwgVHJhaW5pbmcgRXJyb3I=">訓練誤差</a>が小さいが<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5rGO5YyW6Kqk5beuIHwgR2VuZXJhbGl6YXRpb24gRXJyb3I=">汎化誤差</a>が大きい状態。正則化やデータ拡張で緩和可能。

## 音声認識 | Automatic Speech Recognition | ASR<a id="6Z+z5aOw6KqN6K2YIHwgQXV0b21hdGljIFNwZWVjaCBSZWNvZ25pdGlvbiB8IEFTUg=="></a>

- 音響<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>から系列ラベル（文字列）への変換問題。典型的にはCTC損失や<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44Oq44Kr44Os44Oz44OI44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgUk5O">RNN</a>-Tを使用。

## コサイン類似度 | Cosine Similarity<a id="44Kz44K144Kk44Oz6aGe5Ly85bqmIHwgQ29zaW5lIFNpbWlsYXJpdHk="></a>

- 2つのベクトルがどれだけ同じ方向を向いているかを数値で表す指標。

- 具体的には、2つのベクトル $u, v$ の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44Kz44K144Kk44Oz6aGe5Ly85bqmIHwgQ29zaW5lIFNpbWlsYXJpdHk=">コサイン類似度</a>とは、 $u, v$のなす角の余弦(下記式)。

  - 方向の近さを評価する。ベクトルの大きさは考慮しない。
    $$
    \\cos(u,v) = \\frac{u \\cdot v}{|u||v|}
    $$

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#VEYtSURGIHwgVGVybSBGcmVxdWVuY3nigJNJbnZlcnNlIERvY3VtZW50IEZyZXF1ZW5jeQ==">TF-IDF</a>や<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-train_eval.md#d29yZDJ2ZWM=">word2vec</a>などの手法である文章をベクトル化した結果(=<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#RW1iZWRkaW5nIHwg5Z+L44KB6L6844G/">Embedding</a>)の比較に用いられる。

## ハイパーパラメータ | Hyperparameter<a id="44OP44Kk44OR44O844OR44Op44Oh44O844K/IHwgSHlwZXJwYXJhbWV0ZXI="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5qmf5qKw5a2m57+SIHwgTWFjaGluZSBMZWFybmluZw==">機械学習</a>の分野(=<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw==">深層学習</a>を含む)で使われる用語で、主に**モデルの学習過程の設定パラメータ**を指す。
- **モデルの構造をとある設定パラメータで若干調整できるようにしている場合にそのパラメータ**も含めて<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OP44Kk44OR44O844OR44Op44Oh44O844K/IHwgSHlwZXJwYXJhbWV0ZXI=">ハイパーパラメータ</a>と呼ぶこともある。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6KiT57e044OH44O844K/IHwgVHJhaW5pbmcgRGF0YQ==">訓練データ</a>から直接学習されるのではなく、人間が事前に設定する。
- 具体例:
  1. **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">学習率</a> (<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5a2m57+S546HIHwgTGVhcm5pbmcgUmF0ZQ==">Learning Rate</a>)**
  1. **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OQ44OD44OB44K144Kk44K6IHwgQmF0Y2ggU2l6ZQ==">バッチサイズ</a> (<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OQ44OD44OB44K144Kk44K6IHwgQmF0Y2ggU2l6ZQ==">Batch Size</a>)**
  1. **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44Ko44Od44OD44KvIHwgRXBvY2g=">エポック</a>数 (<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44Ko44Od44OD44KvIHwgRXBvY2g=">Epoch</a>s)**
  1. **モデル構造**
  1. **正則化パラメータ**
  1. **<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#T3B0aW1pemVyIHwg5pyA6YGp5YyW5omL5rOV">Optimizer</a>の設定**
  1. **ランダムシード**
