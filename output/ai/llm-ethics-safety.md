<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-ethics-safety.md# -->

## ハルシネーション | Hallucination<a id="44OP44Or44K344ON44O844K344On44OzIHwgSGFsbHVjaW5hdGlvbg=="></a>

- 入力に存在しない情報をあたかも事実のように生成する、生成モデル特有の誤り。

## Prompt Injection<a id="UHJvbXB0IEluamVjdGlvbg=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#44OX44Ot44Oz44OX44OIIHwgUHJvbXB0">プロンプト</a>内に意図的な命令を埋め込み、モデルの出力意図的に改変させる攻撃手法。セキュリティリスクの一種。

## アライメント | Alignment<a id="44Ki44Op44Kk44Oh44Oz44OIIHwgQWxpZ25tZW50"></a>

- モデル出力が人間の意図や社会的規範と整合するよう調整・訓練する取り組み全般。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-training.md#UkxIRiB8IFJlaW5mb3JjZW1lbnQgTGVhcm5pbmcgZnJvbSBIdW1hbiBGZWVkYmFjaw==">RLHF</a>や安全性評価が含まれる。

## ガードレール | Guardrails<a id="44Ks44O844OJ44Os44O844OrIHwgR3VhcmRyYWlscw=="></a>

- モデル出力のセーフティやポリシー制御を目的としたルールやフィルタ。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-ethics-safety.md#44OP44Or44K344ON44O844K344On44OzIHwgSGFsbHVjaW5hdGlvbg==">ハルシネーション</a>回避、有害発言の抑制、特定タスク制限などの用途に活用される。
