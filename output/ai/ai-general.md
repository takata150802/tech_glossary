<!-- 記事URL:https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md# -->

## 人工知能 | Artificial Intelligence | AI<a id="5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ=="></a>

- 人間の知的な行動を模倣するコンピュータシステム
- 2010代~2024年現在まで続く<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a>ブームは第3次<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a>ブーム(3rd Wave)とされている。
  - First Wave (1950s-1970s):
    - GOF<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a>（Good Old Fashioned <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a>；古き良き<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">人工知能</a>）
    - 現在に至る<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a>研究の基礎となるフレームワークが輩出:
      - シンボリック<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a>の一般問題解決器GPS（General
        Problem Solver）
      - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>におけるパーセプトロン
        - 引用元: https://ja.wikipedia.org/wiki/甘利俊一
          - 甘利俊一は連続体力学、情報理論、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>などを研究してきた。1967年、多層パーセプトロンの<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#56K6546H55qE5Yu+6YWN6ZmN5LiL5rOVIHwgU0dE">確率的勾配降下法</a>を考えて定式化に成功したが、この早すぎた発見は当時の計算機の能力の低さもあり検証が難しく、あまり注目されずに終わった。しかし、1986年にデビッド・ラメルハート、ジェフリー・ヒントン、ロナルド・J・ウィリアムスが、この方法を再発見し、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6Kqk5beu6YCG5Lyd5pKt5rOVIHwg44OQ44OD44Kv44OX44Ot44OR44Ky44O844K344On44OzIHwgQmFja3Byb3BhZ2F0aW9u">誤差逆伝播法</a>として発表した事で、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#44OL44Ol44O844Op44Or44ON44OD44OI44Ov44O844KvIHwgTmV1cmFsIE5ldHdvcms=">ニューラルネットワーク</a>研究の第2次ブームへと繋がっている。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5Yu+6YWN5raI5aSx5ZWP6aGMIHwgVmFuaXNoaW5nIEdyYWRpZW50IFByb2JsZW0=">勾配消失問題</a>などの技術的困難があり、この第2次ブームは終焉を迎えたが、その後のディープラーニングブームへと続く礎にもなった。
      - フレーム問題
      - 意味ネットワーク, フレーム理論
      - ファジー集合
      - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#6Ieq54S26KiA6Kqe5Yem55CGIHwgTkxQ">自然言語処理</a>システム SHRDLU
      - 人工無能システム ELIZA
      - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">人工知能</a>用プログラミング言語 LISP
      - 第1回<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">人工知能</a>国際会議 IJC<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a> 1969 @ワシントン
    - この後最初の「<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a> の冬」時代へ
  - Second Wave (1980s-1990s):
    - 人間の専門家の知識を持った <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a> が専門家のような<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5o6o6KuWIHwgSW5mZXJlbmNl">推論</a>を行うエキスパートシステム（Expert System）
      - 人間の専門家の知識を引き出すタスクは知識エンジニアリングと呼ばれ，知識エンジニアという職業まで生まれた
    - 病気の診断を行う MYCIN（マイシン）
    - 油田の推定を行うディップメーターアドバイザー
    - コンピュータの構成を行う XCON（エックスコン）
    - 日本 通産省: 第五世代コンピュータプロジェクト,1982年~
      - 引用元: https://museum.ipsj.or.jp/computer/other/0002.html
        - 通産省は1982年に第五世代コンピュータプロジェクトをスタートさせた．このプロジェクトは，国際貢献を果たしつつ技術先進国として発展するという我が国の政策のもとに始められたもので，国際的にみても創造的・先駆的な技術という意味を込めて「第五世代コンピュータ」と名付けられた．技術目標を「知識情報処理を指向した新しいコンピュータ技術の研究開発」と定め，推進母体として新世代コンピュータ技術開発機構（ICOT）が設立され，技術目標に含まれる多くの要素技術の実証・評価を行うために，並列<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5o6o6KuWIHwgSW5mZXJlbmNl">推論</a>型コンピュータのプロトタイプシステムの試作が行われた．このシステムは，知識情報処理指向のコンピュ−タとしては，世界最高速，かつ，最大規模のものであった．このシステムは，並列<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5o6o6KuWIHwgSW5mZXJlbmNl">推論</a>マシン（PIM）と呼ばれる大規模な並列ハードウェアシステムを持ち，PIM は512台の要素プロセッサからなるPIM/pや256台の要素プロセッサを持つPIM/mなど5つのモデルが作られた．11年間で約540億を投じ1992年度をもって終了した．現在，PIM/pおよびPIM/mは国立科学博物館で保存されている．また，下記のWebで「第五世代博物館」が公開されている．
          - <a href="http://www.jipdec.or.jp/archives/icot/ARCHIVE/HomePage-J.html">http://www.jipdec.or.jp/archives/icot/ARCHIVE/HomePage-J.html</a>
    - 暗黙知の顕在化によるエキスパートシステムの限界などにより，15 年に及ぶ <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a> 冬の時代へ。この間における主要な進歩は下記の通り：
      - ベイジアンネットワーク | Bayesian Network
      - 遺伝的アルゴリズム | Genetic Algorithm
      - 遺伝的プログラミング | Genetic Programming
      - マルチエージェントシステム | Multi-agent System
      - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5by35YyW5a2m57+SIHwgUmVpbmZvcmNlbWVudCBMZWFybmluZw==">強化学習</a>
      - 統計的<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5qmf5qKw5a2m57+SIHwgTWFjaGluZSBMZWFybmluZw==">機械学習</a>, サポ―トベクターマシン
      - データマイニング | Data Minig
  - Third Wave (2010s-Present):
    - コンピューター ハードウェアの進歩(とGPGPUの活用)と大規模なデータセットによる<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw==">Deep Learning</a>のブレークスルー
      - 画像認識コンペティションILSVRC2012でトロント大のAlexNetがエラー率 15.3% で優勝し、次点よりも 10.8% 以上低く<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-overview.md#5rex5bGk5a2m57+SIHwgRGVlcCBMZWFybmluZw==">Deep Learning</a>が広く認知されるきっかけとなった。
        - `Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "ImageNet classification with deep convolutional neural networks." Communications of the ACM 60.6 (2017): 84-90.`
        - ILSVRC2012の次点は東大知能機械 原田研究室の牛久祥孝氏。「ヒントンに敗れた男」として日本経済新聞の記事で紹介されるもお名前がtypo…
    - Google、Meta(旧Facebook)、Microsoft、Amazon等の北米IT大手が研究開発を牽引し、多くのプロダクト・サービスに<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Lq65bel55+l6IO9IHwgQXJ0aWZpY2lhbCBJbnRlbGxpZ2VuY2UgfCBBSQ==">AI</a>が投入され商業的成功を収めている

## 機械学習 | Machine Learning<a id="5qmf5qKw5a2m57+SIHwgTWFjaGluZSBMZWFybmluZw=="></a>

- 明示的なルール記述なしにデータからパターンを学習するアルゴリズム全般。

## 強化学習 | Reinforcement Learning<a id="5by35YyW5a2m57+SIHwgUmVpbmZvcmNlbWVudCBMZWFybmluZw=="></a>

- エージェントが環境からの報酬を最大化する行動方策を学習。Q学習やポリシー勾配法がある。

## 特徴量 | Feature<a id="54m55b606YePIHwgRmVhdHVyZQ=="></a>

- 学習モデルの入力ベクトルを構成する次元。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#5Li75oiQ5YiG5YiG5p6QIHwgUHJpbmNpcGFsIENvbXBvbmVudCBBbmFseXNpcyB8IFBDQQ==">PCA</a>やオートエンコーダーで<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5qyh5YWD5YmK5ribIHwgRGltZW5zaW9uYWxpdHkgUmVkdWN0aW9u">次元削減</a>可能。

## 次元の呪い | Curse of Dimensionality<a id="5qyh5YWD44Gu5ZGq44GEIHwgQ3Vyc2Ugb2YgRGltZW5zaW9uYWxpdHk="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>の次元を増やしすぎると、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5qmf5qKw5a2m57+SIHwgTWFjaGluZSBMZWFybmluZw==">機械学習</a>モデルの精度が劣化する現象
  - 経験的に<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5qmf5qKw5a2m57+SIHwgTWFjaGluZSBMZWFybmluZw==">機械学習</a>モデルの学習に必要なデータ量は<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>の次元の指数オーダーで増大し、現実的にそのようなデータ量を集めるのが不可能となるため
  - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#55CD6Z2i6ZuG5Lit54++6LGhIHwgQ29uY2VudHJhdGlvbiBvbiB0aGUgc3BoZXJl">球面集中現象</a>:
    - <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>のノルムが一様となり、意味をなさなくなるため(1万次元の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>のうち、1番目の次元の値が多少違っても1万次元全体のノルムには全く見えてこない)
    - 標本平均が母平均に一致しなくなるため(1万次元の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>のうち、1番目の次元の値だけ見ると標本平均は母平均に一致するが(=大数の法則)、1万次元の標本平均と1万次元の母平均のノルムは全く一致しない)

## 球面集中現象 | Concentration on the sphere<a id="55CD6Z2i6ZuG5Lit54++6LGhIHwgQ29uY2VudHJhdGlvbiBvbiB0aGUgc3BoZXJl"></a>

- 次元数の増加に伴い、ある点から見たときの他の点はその点から遠ざかり、また同じような距離に存在するようになる現象。
  - d次元の空間に、ある体積ごとに同じ数の点が分布しているとする
  - この時、ある点を中心として半径がそれぞれrとar(0\<a\<1)のd次元超球S1, S2を考えたとき、(S1の体積 - S2の体積)/(S1の体積)は1 - a^dとなり、dの増加にともない1に近づく
  - つまり、S1の体積のほとんどはS1とS2の超球の隙間に存在する。よって、d次元空間の点もS1とS2の超球の隙間に存在することになる。

## 分類 | Classification<a id="5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24="></a>

- 離散ラベルを予測する<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pWZ5bir44GC44KK5a2m57+SIHwgU3VwZXJ2aXNlZCBMZWFybmluZw==">教師あり学習</a>問題。多クラスではSoftmaxとクロスエントロピーを使用。

## 回帰 | Regression<a id="5Zue5biwIHwgUmVncmVzc2lvbg=="></a>

- 連続値を予測対象とする<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pWZ5bir44GC44KK5a2m57+SIHwgU3VwZXJ2aXNlZCBMZWFybmluZw==">教師あり学習</a>。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pCN5aSx6Zai5pWwIHwgTG9zcyBGdW5jdGlvbg==">損失関数</a>としてMSEやMAEが用いられる。

## 確率的勾配降下法 | SGD<a id="56K6546H55qE5Yu+6YWN6ZmN5LiL5rOVIHwgU0dE"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#44OQ44OD44OB44K144Kk44K6IHwgQmF0Y2ggU2l6ZQ==">バッチサイズ</a>1または小バッチでパラメータ更新を行う<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5Yu+6YWN6ZmN5LiL5rOVIHwgR3JhZGllbnQgRGVzY2VudA==">勾配降下法</a>。計算効率と汎化に優れる。

## 訓練データ | Training Data<a id="6KiT57e044OH44O844K/IHwgVHJhaW5pbmcgRGF0YQ=="></a>

- モデルのパラメータ最適化に使用されるデータセット。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-train_eval.md#5Lqk5beu5qSc6Ki8IHwgQ3Jvc3MtVmFsaWRhdGlvbg==">交差検証</a>による分割が一般的。

## 訓練誤差 | Training Error<a id="6KiT57e06Kqk5beuIHwgVHJhaW5pbmcgRXJyb3I="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6KiT57e044OH44O844K/IHwgVHJhaW5pbmcgRGF0YQ==">訓練データ</a>に対する<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pCN5aSx6Zai5pWwIHwgTG9zcyBGdW5jdGlvbg==">損失関数</a>の平均値。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5rGO5YyW6Kqk5beuIHwgR2VuZXJhbGl6YXRpb24gRXJyb3I=">汎化誤差</a>とのギャップが<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YGO5a2m57+SIHwgT3ZlcmZpdHRpbmc=">過学習</a>の兆候。

## 汎化誤差 | Generalization Error<a id="5rGO5YyW6Kqk5beuIHwgR2VuZXJhbGl6YXRpb24gRXJyb3I="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6KiT57e044OH44O844K/IHwgVHJhaW5pbmcgRGF0YQ==">訓練データ</a>外の新規入力に対する誤差。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-train_eval.md#5Lqk5beu5qSc6Ki8IHwgQ3Jvc3MtVmFsaWRhdGlvbg==">交差検証</a>や正則化で抑制。

## F値 | F1 Score<a id="RuWApCB8IEYxIFNjb3Jl"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6YGp5ZCI546HIHwgUHJlY2lzaW9u">適合率</a>と<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YaN54++546HIHwgUmVjYWxs">再現率</a>の調和平均。不均衡デタにおける<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>性能のバランス指標。

## 混同行列 | Confusion Matrix<a id="5re35ZCM6KGM5YiXIHwgQ29uZnVzaW9uIE1hdHJpeA=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>モデルの性能評価指標。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#6YGp5ZCI546HIHwgUHJlY2lzaW9u">適合率</a>、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YaN54++546HIHwgUmVjYWxs">再現率</a>、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#RuWApCB8IEYxIFNjb3Jl">F値</a>はここから導出。

## 交検証 | Cross Validation<a id="5Lqk5qSc6Ki8IHwgQ3Jvc3MgVmFsaWRhdGlvbg=="></a>

- データを複数分割し、訓練と検証を繰り返すことでモデルの汎化性能を推定する手法。

## 再現率 | Recall<a id="5YaN54++546HIHwgUmVjYWxs"></a>

- 実際の正例のうち、正しく予測された割合。感度とも。

## ROC曲線 | Receiver Operating Characteristic Curve<a id="Uk9D5puy57eaIHwgUmVjZWl2ZXIgT3BlcmF0aW5nIENoYXJhY3RlcmlzdGljIEN1cnZl"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>モデルの閾値を変化させたときのTPRと<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-train_eval.md#5YG96Zm95oCnIHwgRmFsc2UgUG9zaXRpdmUgfCBGUA==">FP</a>Rの関係を可視化する評価標。

## Jaccard係数 | Jaccard Index<a id="SmFjY2FyZOS/guaVsCB8IEphY2NhcmQgSW5kZXg="></a>

- 集合間の類似度を表す指標で、IoUとも呼ばれる。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#54mp5L2T5qSc5Ye6IHwgT2JqZWN0IERldGVjdGlvbg==">物体検出</a>の評価にも用いられる。

## KL Divergence<a id="S0wgRGl2ZXJnZW5jZQ=="></a>

- ある確率分布が別の分布かられだけ異なるかを測る非対称な距離指標。言語モデルの<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm-training.md#5LqL5YmN5a2m57+SIHwgUHJlLXRyYWluaW5n">事前学習</a>にも使用。

## オッズ比 | Odds Ratio<a id="44Kq44OD44K65q+UIHwgT2RkcyBSYXRpbw=="></a>

- 2つの事象の相対的な発生確率比。ロジスティック<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Zue5biwIHwgUmVncmVzc2lvbg==">回帰</a>での係数解釈に用いられる。

## ロジット関数 | Logit Function<a id="44Ot44K444OD44OI6Zai5pWwIHwgTG9naXQgRnVuY3Rpb24="></a>

- 確率値をオッズの対数に変換する写像。ロジスティック<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Zue5biwIHwgUmVncmVzc2lvbg==">回帰</a>の決定関数で使用。

## 最小二乗法 | Least Square Method<a id="5pyA5bCP5LqM5LmX5rOVIHwgTGVhc3QgU3F1YXJlIE1ldGhvZA=="></a>

- 目的変数Yを説明変数Xの線形モデルで表したときの二乗誤差($=\\epsilon^2$)を最小にするパラメータ$\\theta$を求める手法
- 線形モデルの誤差$\\epsilon$を多変量正規分布と仮定したときの<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5pyA5bCk5o6o5a6aIHwg5pyA5bCk5o6o5a6a5rOVIHwgTWF4aW11bSBMaWtlbGlob29kIEVzdGltYXRpb24=">最尤推定</a>に相当する

```math
\begin{align}
線形モデル&: Y = X\theta + \epsilon \\ 
目的変数&: Y \\
説明変数&: X \\
パラメータ&: \theta \\
誤差&: \epsilon \\
推定値&: \hat{\theta} = (X^TX)^{-1}X^TY ・・・①
\end{align}
```

**① 推定値 $\\hat{\\theta}$の導出:**

```math
\begin{align}
&\begin{split}
\epsilon^2 &=(Y-X\theta)^T(Y-X\theta) \\
&=Y^TY - Y^TX\theta - \theta^TX^TY + \theta^TX^TX\theta \\
\end{split} \\
&\begin{split}
\frac{\partial }{\partial \theta} \epsilon^2 &= \frac{\partial }{\partial \theta} Y^TY - \frac{\partial }{\partial \theta} Y^TX\theta - \frac{\partial }{\partial \theta} \theta^TX^TY + \frac{\partial }{\partial \theta} \theta^TX^TX\theta \\
&= 0 + \frac{\partial }{\partial \theta} Y^TX\theta - \frac{\partial }{\partial \theta} (X^TY)^T\theta + \frac{\partial }{\partial \theta} \theta^TX^TX\theta \\
&= -2 \frac{\partial }{\partial \theta} Y^TX\theta + \frac{\partial }{\partial \theta} \theta^TX^TX\theta ・・・② \\
&= -2X^TY+2X^TX\theta ・・・③ \\
\end{split}
\end{align}
```

したがって、$\\frac{\\partial }{\\partial \\theta} \\epsilon^2 = 0$となるような$\\theta$は、

```math
\begin{align}
0 &= -2X^TY+2X^TX\theta \\
X^TX\theta &= X^TY \\
(X^TX)^{-1}X^TX\theta &= (X^TX)^{-1}X^TY \\
\theta &= (X^TX)^{-1}X^TY ・・・上述の①の式
\end{align}
```

なお②から③の式変形は、

```math
\begin{align}
第１項&: - 2\frac{\partial }{\partial \theta} Y^TX\theta = - 2 (Y^TX)^T = -2 X^TY \\
第2項&: \frac{\partial }{\partial \theta} \theta^TX^TX\theta = 2X^TX\theta 
\end{align}
```

## 尤度 | 尤度関数 | Likelihood | Likehood function<a id="5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u"></a>

- 観測値がxとなる確率を何らかの確率モデルp(x; θ)で表現しようとしている状況を考える。
- 例えば観測値として{x_1, x_2, ..., x_n}が得られたとき、何らかの確率モデルp(x; θ)が本当に正しいなら、この観測値{x_1, x_2, ..., x_n}が確率モデルのパラメータθの下で生じる確率は下記式で書ける。これが<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>。

```math
尤度: L(X = \{x_1, x_2, ..., x_n\};\theta) = \Pi_{i=1}^N p(x_i;\theta) 
```

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>とは**確率モデルのパラメータθの尤もらしさ**を表す関数。具体的には、確率モデルとそのパラメータθが本当に正しいなら実際に観測された観測値系列の発生確率がどれくらいになるかを計算する関数。これは、パラメータθが尤もらしいなら今回の観測値系列の発生確率(=<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>)は実際に観測されたものであるので当然高くなるはずで、逆に<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>が低ければそのパラメータθは今回の観測値系列を上手く表現できておらず尤もらしくないという考え方に基づいている。

**具体例1:**

- 観測値はコイン投げの裏表で、コインが表になる確率を下記の確率モデルp(x; θ)で表現しようとしている状況を考える。
- 例えば観測値として{裏, 表}が得られたとき、下記の確率モデルp(x; θ)が本当に正しいなら、この観測値{裏, 表}が確率モデルのパラメータθの下で生じる確率は下記式で書ける(=<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>)。

```math
確率モデル: p(x; \theta) = 
\begin{cases}
  \theta  \text{\ \ \ \ \ \ \ \ ※\ x\ ==\ 表の場合} \\\
  1 - \theta \text{※\ x\ ==\ 裏の場合}
\end{cases}
```

```math
尤度: L(X=\{裏, 表\};\theta) = p(裏;\theta) \cdot p(表;\theta) = (1 -\theta) \cdot \theta
```

- なお、上記の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>を最大化するのはθ=0.5である(0≦θ≦1で(1-θ)θはθ=0.5で最大となるので)。

**具体例2:**

- 観測値は<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>の出力<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#44OI44O844Kv44OzIHwgVG9rZW4=">トークン</a>で、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>がある出力<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#44OI44O844Kv44OzIHwgVG9rZW4=">トークン</a>を生成する確率を確率モデル<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>(入力<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#44OI44O844Kv44OzIHwgVG9rZW4=">トークン</a>列; 出力<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#44OI44O844Kv44OzIHwgVG9rZW4=">トークン</a>; θ)で表現しようとしている状況を考える。
  - θは<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YeN44G/IHwg6YeN44G/44OR44Op44Oh44O844K/IHwgV2VpZ2h0IHwgV2VpZ2h0IFBhcmFtZXRlcg==">重みパラメータ</a>となる。
- 例えば観測値として{日本の首都は?→東京, 日本の首都は?東京→です。}が得られたとき、確率モデル<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>(入力<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#44OI44O844Kv44OzIHwgVG9rZW4=">トークン</a>列; 出力<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/nlp.md#44OI44O844Kv44OzIHwgVG9rZW4=">トークン</a>; θ)が本当に正しいなら、この観測値{日本の首都は?→東京, 日本の首都は?東京→です。}が確率モデルのパラメータθの下で生じる確率は下記式で書ける。

```math
尤度: L(X=\{日本の首都は?→東京, 日本の首都は?東京→です。\};\theta) \\\
= LLM(日本の首都は?;東京;\theta) \cdot LLM(日本の首都は?東京;です。;\theta)
```

- なお、上記の<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>を最大化するθを求めるのが<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/llm.md#5aSn6KaP5qih6KiA6Kqe44Oi44OH44OrfCBMYXJnZSBMYW5ndWFnZSBNb2RlbCB8IExMTQ==">LLM</a>の学習に相当する。ただ、代数的に求めることができないので数値解析的に求める必要があり、具体的には<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6Kqk5beu6YCG5Lyd5pKt5rOVIHwg44OQ44OD44Kv44OX44Ot44OR44Ky44O844K344On44OzIHwgQmFja3Byb3BhZ2F0aW9u">誤差逆伝播法</a>が用いられる。

## 最尤推定 | 最尤推定法 | Maximum Likelihood Estimation<a id="5pyA5bCk5o6o5a6aIHwg5pyA5bCk5o6o5a6a5rOVIHwgTWF4aW11bSBMaWtlbGlob29kIEVzdGltYXRpb24="></a>

- 観測値がxとなる確率を何らかの確率モデルp(x; θ)で表現しようとしている状況で、最適なパラメータθを推定する手法。
- 具体的には、観測値系列{x_1, x_2, ..., x_n}が得られたとき、確率モデルp(x; θ)とそのパラメータθが正しいなら、その観測値系列の発生確率は確率モデルのパラメータθの関数となる(=<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>)。

```math
尤度: L(X = \{x_1, x_2, ..., x_n\};\theta) = \Pi_{i=1}^N p(x_i;\theta) 
```

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5pyA5bCk5o6o5a6aIHwg5pyA5bCk5o6o5a6a5rOVIHwgTWF4aW11bSBMaWtlbGlob29kIEVzdGltYXRpb24=">最尤推定</a>とは、この<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>を最大化するパラメータθの値を以ってθの推定値とする手法。
- これは、パラメータθが尤もらしいなら今回の観測値系列の発生確率(=<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>)は実際に観測されたものであるので当然高くなるはずで、逆に<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5bCk5bqmIHwg5bCk5bqm6Zai5pWwIHwgTGlrZWxpaG9vZCB8IExpa2Vob29kIGZ1bmN0aW9u">尤度</a>が低ければそのパラメータθは今回の観測値系列を上手く表現できておらず尤もらしくないという考え方に基づいている。

## 標本誤差 | Sampling Error<a id="5qiZ5pys6Kqk5beuIHwgU2FtcGxpbmcgRXJyb3I="></a>

- 母集団と標本の間で推定値が異なる誤差。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-train_eval.md#5Lqk5beu5qSc6Ki8IHwgQ3Jvc3MtVmFsaWRhdGlvbg==">交差検証</a>で統計的に評価可能。

## 標準化 | Standardization<a id="5qiZ5rqW5YyWIHwgU3RhbmRhcmRpemF0aW9u"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#54m55b606YePIHwgRmVhdHVyZQ==">特徴量</a>のスケールを平均0、分散1に<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5q2j6KaP5YyWIHwgUmVndWxhcml6YXRpb24=">正規化</a>する前処理。勾配安定性と収束性向上に寄与。

## 次元削減 | Dimensionality Reduction<a id="5qyh5YWD5YmK5ribIHwgRGltZW5zaW9uYWxpdHkgUmVkdWN0aW9u"></a>

- 高次元特徴空間を情報保持しつつ低次元へ射影する処理。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#5Li75oiQ5YiG5YiG5p6QIHwgUHJpbmNpcGFsIENvbXBvbmVudCBBbmFseXNpcyB8IFBDQQ==">PCA</a>、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/dl-train_eval.md#dC1TTkUgfCB0LURpc3RyaWJ1dGVkIFN0b2NoYXN0aWMgTmVpZ2hib3IgRW1iZWRkaW5n">t-SNE</a>、UMAPなど。

## 正規化 | Regularization<a id="5q2j6KaP5YyWIHwgUmVndWxhcml6YXRpb24="></a>

- モデルの<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YGO5a2m57+SIHwgT3ZlcmZpdHRpbmc=">過学習</a>を防ぐため、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#5pCN5aSx6Zai5pWwIHwgTG9zcyBGdW5jdGlvbg==">損失関数</a>にペナルティ項を加える手法。L1, L2正則化など。

## 自己組織化マップ | Self-Organizing Map<a id="6Ieq5bex57WE57mU5YyW44Oe44OD44OXIHwgU2VsZi1Pcmdhbml6aW5nIE1hcA=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#5pWZ5bir44Gq44GX5a2m57+SIHwgVW5zdXBlcnZpc2VkIExlYXJuaW5n">教師なし学習</a>による高次元データの位相保存写像。Kohonenマップとも。

## 適合率 | Precision<a id="6YGp5ZCI546HIHwgUHJlY2lzaW9u"></a>

- 予測した正例のうち、実際に正しかった割合。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#RuWApCB8IEYxIFNjb3Jl">F値</a>に影響。

## 重回帰 | Multiple Regression<a id="6YeN5Zue5biwIHwgTXVsdGlwbGUgUmVncmVzc2lvbg=="></a>

- 複数の説明変数から連続目的変数を予測する<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5Zue5biwIHwgUmVncmVzc2lvbg==">回帰</a>手法。

## EMアルゴリズム | Expectation-Maximization<a id="RU3jgqLjg6vjgrTjg6rjgrrjg6AgfCBFeHBlY3RhdGlvbi1NYXhpbWl6YXRpb24="></a>

- 隠れ変数のある確率モデルのパラメータ推定手法で、EステップとMステップを交互に適用。

## サポートベクターマシン | SVM<a id="44K144Od44O844OI44OZ44Kv44K/44O844Oe44K344OzIHwgU1ZN"></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44Kr44O844ON44Or44OI44Oq44OD44KvIHwgS2VybmVsIFRyaWNr">カーネルトリック</a>を利用して高次元空間でマージン最大化を行う線形<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5YiG6aGeIHwgQ2xhc3NpZmljYXRpb24=">分類</a>器。

## カーネルトリック | Kernel Trick<a id="44Kr44O844ON44Or44OI44Oq44OD44KvIHwgS2VybmVsIFRyaWNr"></a>

- 非線形特徴変換を暗黙的に高次元空間で内計算する手法。<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44K144Od44O844OI44OZ44Kv44K/44O844Oe44K344OzIHwgU1ZN">SVM</a>や<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ml-overview.md#5Li75oiQ5YiG5YiG5p6QIHwgUHJpbmNpcGFsIENvbXBvbmVudCBBbmFseXNpcyB8IFBDQQ==">PCA</a>で活用。

## ランダムフォレスト | Random Forest<a id="44Op44Oz44OA44Og44OV44Kp44Os44K544OIIHwgUmFuZG9tIEZvcmVzdA=="></a>

- 多数の定木をバギングにより構築し、多数決で出力を得るアンサンブル学習法。

## バイアス | Bias<a id="44OQ44Kk44Ki44K5IHwgQmlhcw=="></a>

- モデルの予測値の平均と真の値との差。
- 複雑なモデルを使用すると<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44OQ44Kk44Ki44K5IHwgQmlhcw==">バイアス</a>は低下するが、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44OQ44Oq44Ki44Oz44K5IHwgVmFyaWFuY2U=">バリアンス</a>が増加する傾向がある。

## バリアンス | Variance<a id="44OQ44Oq44Ki44Oz44K5IHwgVmFyaWFuY2U="></a>

- モデルの予測値の分散。
- 学習データセットの変動に対するモデルの感度を示す。
- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#44OQ44Oq44Ki44Oz44K5IHwgVmFyaWFuY2U=">バリアンス</a>が高い場合、単一の学習データセットに<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/deep-learning.md#6YGO5a2m57+SIHwgT3ZlcmZpdHRpbmc=">過学習</a>している可能性がある。

## 削減不能な誤差 | Irreducible Error<a id="5YmK5rib5LiN6IO944Gq6Kqk5beuIHwgSXJyZWR1Y2libGUgRXJyb3I="></a>

- データの測定誤差などに由来するノイズの分散。
- 削減できない。

## 条件付き確率 | Conditional Probability<a id="5p2h5Lu25LuY44GN56K6546HIHwgQ29uZGl0aW9uYWwgUHJvYmFiaWxpdHk="></a>

- ある事象が起こったときの別の事象の確率。

```math
  Bが起こったときのAの条件付き確率: P(A|B) = \frac{P(A \cap B)}{P(B)}
```

```math
  なお、事象AとBが独立ならP(A \cap B) = P(A)P(B)、つまりP(A|B) = P(A)という簡単な話になる。そして、独立でない場合を考えたいとき、P(A \cap B) = P(A|B)P(B)を満たすP(A|B)なる確率が必要になり、このP(A|B)を条件付き確率と呼んでいるという話。
```

## 同時確率| Joint probability<a id="5ZCM5pmC56K6546HfCBKb2ludCBwcm9iYWJpbGl0eQ=="></a>

- 複数の事象が同時に起きる確率

```math
事象AとBの同時確率: P(A \cap B)
```

## 条件付き確率の連鎖律 | Chain rule<a id="5p2h5Lu25LuY44GN56K6546H44Gu6YCj6Y6W5b6LIHwgQ2hhaW4gcnVsZQ=="></a>

- <a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5ZCM5pmC56K6546HfCBKb2ludCBwcm9iYWJpbGl0eQ==">同時確率</a>は、<a href="https://github.com/takata150802/tech_glossary/blob/main/output/ai/ai-general.md#5p2h5Lu25LuY44GN56K6546HIHwgQ29uZGl0aW9uYWwgUHJvYmFiaWxpdHk=">条件付き確率</a>の積で表現できる。

```math
  事象AとBとCの同時確率: P(A \cap B \cap C) = P(A|B \cap C)P(B \cap C) = P(A|B \cap C)P(B|C)P(C)
```

# 参考文献

- 朱鷺の杜Wiki https://ibisforest.org/index.php?FrontPage
- https://www.kamishima.net/archive/mldm-overview.pdf
- https://www.ieice-hbkb.org/files/S3/S3gun_03hen_01.pdf
- https://twitter.com/losnuevetoros/status/1168326617023700992
